{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from common.math_functions import *\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 誤差逆伝搬法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p123] なお、誤差逆伝播法を計算グラフによって説明するアイデアは、Andrej Karpathy のブログ [4] 、また、彼と Fei-Fei Li 教授らによって行われたスタンフォード大学の ディープラーニングの授業「CS231n」[5] を参考にしています。\n",
    "\n",
    "[Andrej Karpathy](https://cs.stanford.edu/people/karpathy/) さんは Tesla の CV 系 AI 研究者で，ブログは[ここ](http://karpathy.github.io/neuralnets/)．PhD の指導教官が Stanford の [Fei-Fei Li](https://profiles.stanford.edu/fei-fei-li?tab=bio) さんで，この方は GoogleCloud で Chief Scientist してたらしい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1. 計算グラフ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p123] 計算グラフとは、計算の過程をグラフによって表したものです。ここで言うグラフ とは、データ構造としてのグラフであり、複数のノードとエッジによって表現されます\n",
    " \n",
    "計算グラフは「演算のフローを可視化したグラフ」という感じ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p124] 計算グラフはノードと矢印によって計算の過程を表します。ノードは○で表記し、 ○の中に演算の内容を書きます。また、計算の途中結果を矢印の上部に書くことで、 ノードごとの計算結果が左から右へ伝わるように表します。\n",
    "\n",
    "前述のニューロンのグラフ (図3-18とか) ではノードが「変数(入力,特徴量,出力)」で矢印が「変換(写像)」というイメージだったが，ここの計算グラフ(図5-3とか)では逆でノードが「演算(変換,写像)」で矢印が「変数(入力,特徴量,出力)」というイメージ．混同しないように．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p126] ここまで見てきたように、計算グラフを使って問題を解くには、\n",
    "1. 計算グラフを構築する\n",
    "2. 計算グラフ上で計算を左から右へ進める\n",
    ">\n",
    ">という流れで行います。ここで 2 番目の「計算を左から右へ進める」というステップは、順方向の伝播、略して、**順伝播**(forward propagation)と言います。順伝播は、計算グラフの出発点から終着点への伝播です。順伝播という名前があるのであれば、逆方向――図で言うと、右から左方向へ――の伝播も考えることができるでしょう。実際、それを**逆伝播**(backward propagation)と言います。逆伝播は、この先、 微分を計算するにあたって重要な働きをします。\n",
    "\n",
    "そもそも計算グラフは「演算の過程を左スタート右ゴールで可視化したもの」なので，左から右へ計算を進める過程を「順伝搬」と呼ぶのは自然．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p127] たとえば、リンゴとそれ以外の買い物を合計する計算―― 4,000 + 200 → 4,200――は、4,000 という数字がどのように計算されてきたかということについては考えずに、ただ 2 つの数字を足せばよいということを意味します。言い換えれば、各ノードの計算で行うべきことは、自分に関係する計算――この例では、入力された 2 つの数字の足し算――だけであり、 全体のことについては何も考えなくてよいのです。\n",
    "このように、計算グラフでは、局所的な計算に集中することができます。たとえ全体の計算がどんなに複雑であったとしても、各ステップでやることは、対象とするノードの「局所的な計算」なのです。局所的な計算は単純ですが、その結果を伝達することで、全体を構成する複雑な計算の結果が得られます。... 計算グラフも、複雑な計算を「単純で局所的な計算」に分割して、流れ 作業を行うように、計算の結果を次のノードへと伝達していきます\n",
    "\n",
    "計算グラフのこのメリットは，関数型プログラミングパラダイムのメリットと似ている．大きい処理を細かい関数に分けることで，各関数の入出力の仕様だけに注意を払って実装とか単体テストができるようになり，気持ちが楽でミスりづらいしデバッグしやすい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p128] ここでは、リンゴの値段に関する微分だけを求めましたが、「消費税に関する支払金額の微分」や「リンゴの個数に関する支払金額の微分」も同様の手順で求めることができます。そして、その際には、途中まで求めた微分(途中まで流れた微分)の結果を共有することができ、効率良く複数の微分を計算することができるのです。\n",
    "\n",
    "ここが back propagation の肝だと思う．途中までの微分の結果を共有できるので，無駄が削られ計算コストが下がる．\n",
    "\n",
    "電気信号回路(ニューロン)でふわっとしたイメージすると...  \n",
    "これまでのナイーブな勾配算出法は，\n",
    "- 第1層の各ニューロンから出る信号をちょっと強めて，最後の信号 (出力層から出た信号をロス関数に入れて出る信号) がどのくらい変化するか見る．それで第1層の偏微分係数ゲット．\n",
    "- 第2層の各ニューロンから出る信号をちょっと強めて，最後の信号がどのくらい変化するか見る．それで第2層の偏微分係数ゲット．\n",
    "- ...\n",
    "\n",
    "という感じ．ここでポイントなのが，例えば第 $n-1$ 中間層と第 $n$層(出力層)の間には，何度も(全パラメータ数分)だけ信号が流れている．  \n",
    "一方，誤差逆伝搬では，\n",
    "\n",
    "- 第 $n-1$ 層の各ニューロンから出る信号をちょっと強めて，最後の信号がどのくらい変化するか見る．それで第 $n-1$ 層の偏微分係数ゲット．そしてその情報を記録してとっておく．\n",
    "- 第 $n-2$ 層の各ニューロンから出る信号をちょっと強めて，次の第 $n-1$ 層に入る信号がどのくらい変化するかを見る．それを先ほど記録した $n-1$ 層の偏微分係数に掛け合わせて，第 $n-2$ 層の変微分係数を取得．この情報も記録しておく．\n",
    "- ...\n",
    "\n",
    "という感じ．連鎖律を使ったことで，例えば先ほどの方法だと何度も信号が流されていた $n-1$ 層と $n$ 層の間に，１回だけしか信号が流れていない．無駄を削ぎ落とせている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2. 連鎖律"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフのノードは\n",
    "\n",
    "- 左から入力された信号に対しては，その値をノード関数で変換して結果を右に出力する\n",
    "- 右から入力された信号に対しては，それにノード関数の微分を掛け合わせて左に出力する\n",
    "\n",
    "という挙動を示す(ことにしている)．そうすると色々便利だから．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p131] 図 5-7 で注目すべきは、一番左の逆伝播の結果です。これは、連鎖律より、\n",
    "...「x に関する z の微分」に対応します。つまり、逆伝播が行っていることは、連鎖律の原理から構成されているのです。\n",
    "\n",
    "もはや計算グラフは「合成関数の微分を分かりやすく求めるためのツール」とも捉えられそう．普通に微積のテストとかでも役立ちそう．\n",
    "\n",
    "少し違っていて注意しないといけないのは， SGD においては「合成関数の導関数」ではなく 「合成関数の微分係数」という１点の値を求めようとしてる，ということ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3. 逆伝搬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p132] なお、この例では上流から伝わった微分の値を $\\frac{\\partial L}{\\partial z}$ としましたが、これは図5-10に示すように、最終的に $L$ という値を出力する大きな計算グラフを想定しているため\n",
    "\n",
    "NN の SGD とかでは「各パラメータをちょっと動かした時にロス $L$ がどのくらい変化するか」という勾配を求めようとするので，計算グラフの最後には必ず，ロス $L$ を出力するロス関数ノードがあるはず．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4. 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 次節では、ニューラルネットワークを構成する「層(レイヤ)」をひとつのクラスで実装することにします。ここで言う「レイヤ」とは、ニューラルネットワー クにおける機能の単位です。たとえば、シグモイド関数のための Sigmoid や、 行列の積のための Affine など、レイヤ単位で実装を行います。そのため、こ こでも「レイヤ」という単位で、乗算ノードと加算ノードを実装します。\n",
    "\n",
    "次節以降 (例えば図5-28) では，当初の俺が思ってた通り変換 (変数じゃなくて) を層 (レイヤー) とみなしている．このグラフでは変換がノードで表されててノード群を層と見るのは視覚的に直感的．この捉え方なら $n$ 層ニューラルネットワークっていう用語も「$n$ 回(層)の変換を行う NN」と解釈できて，分かりやすい．\n",
    "\n",
    "そう考えると，前述の図3-18みたいな「変数がノードで変換が矢印」のグラフより，後述の図5-14~5-30みたいな「変数が矢印で変換がノード」のグラフの方が「層(レイヤー)」のイメージがつきやすくて良いかも．まあ，どっちの見方もできるようにしとくのが良い．\n",
    "\n",
    "計算グラフは NN を実装する上での良い設計図になっているってことか．ノード(あるいは複数ノードをまとめたレイヤー)を class で実装して，そのインスタンスを組み合わせることで NN 全体を実装できるから．Pytorch もそんな感じだった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗算レイヤの実装 (5.4.1 / p137)\n",
    "\n",
    "class MulLayer:\n",
    "    \"\"\"掛け算レイヤを表すクラス\n",
    "    \n",
    "    f(x,y) = xy という変換に対応するクラス．\n",
    "    計算グラフのノードをそのまま実装するイメージ．\n",
    "    \n",
    "    Attributes:\n",
    "      x: 掛け算関数f(x,y) = x*y の x．\n",
    "      y: 同じく． のちの NN では，更新が繰り返されるパラメータ値が状態として保存されている．\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \n",
    "        アトリビュートの初期化を __init__() ではなく forward() メソッドでやる理由\n",
    "        ニューラルネットで逆伝搬する時のフローをイメージしてる．\n",
    "        １回だけ予測値とロスを算出して(forward して)，\n",
    "        そこから戻りながら勾配を計算していく(backward する)というフローが採られるので，\n",
    "        それを想定して最初の forward 時にアトリビュートの初期化が起こるように書いてある．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"掛け算レイヤの forward 処理\n",
    "        \n",
    "        つまり，ただ掛け算を実行して結果を出力する．\n",
    "        ここでアトリビュート(状態)の初期化や更新が行われる理由は，↑を参照．\n",
    "        \n",
    "        Returns:\n",
    "          - self.x * self.y\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"掛け算レイヤの backward 処理\n",
    "        \n",
    "        つまり，状態 x,y に保存されている値での掛け算関数の勾配を計算する．\n",
    "        つまり，座標(self.x, self.y)における掛け算関数の勾配を計算する．\n",
    "        さらに(計算グラフ上の右から)入力された値 dout に算出された勾配を掛け合わせる．\n",
    "        そしてそれを出力する．\n",
    "        \n",
    "        Returns:\n",
    "          - self.x における f(x,y)=x*y の偏微分係数(勾配)\n",
    "          - self.y における f(x,y)=x*y の偏微分係数(勾配)        \n",
    "        \n",
    "        \"\"\"\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100 2\n",
      "220.00000000000003\n",
      "200 1.1\n",
      "1.1 200\n",
      "2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get layer\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "# 各レイヤ(計算グラフのノード)ごとのオブジェクトを作成．\n",
    "\n",
    "\n",
    "# forward\n",
    "\n",
    "apple_price = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "apple_sum = mul_apple_layer.forward(x=apple_price, y=apple_num)\n",
    "print(apple_sum)  # forward 伝搬での出力\n",
    "print(mul_apple_layer.x, mul_apple_layer.y)  # 状態に forward 伝搬での入力が記録されている\n",
    "\n",
    "total_pay = mul_tax_layer.forward(x=apple_sum, y=tax)\n",
    "print(total_pay)  # forward 伝搬での出力\n",
    "print(mul_tax_layer.x, mul_tax_layer.y)  # 状態に forward 伝搬での入力が記録されてる\n",
    "\n",
    "\n",
    "# backward\n",
    "\n",
    "dtotal_pay = 1\n",
    "# いまの合成関数全体の最終出力は total_pay で，\n",
    "# backward に　chain rule で勾配を求めていく都合上，\n",
    "# 最初に d(total_pay) / d(total_pay) という自分自身による微分係数(常に１)を作っとく必要がある．\n",
    "\n",
    "dapple_sum, dtax = mul_tax_layer.backward(dtotal_pay)\n",
    "print(dapple_sum, dtax)\n",
    "# apple_sum, tax の(forward 時に属性に記録した値での) (total_pay に対する) 偏微分係数を算出．\n",
    "# これはノードから左方向(逆方向)に吐き出されるイメージ．\n",
    "# d(total_pay) / d(apple_sum)  と  d(total_pay) / d(tax)　という感じ．\n",
    "# 最終出力(目的関数値) total_pay に直接影響してる変数なので， chain rule は活用してない．\n",
    "\n",
    "dapple_price, dapple_num = mul_apple_layer.backward(dapple_sum)\n",
    "print(dapple_price, dapple_num)\n",
    "# apple_price, apple_num の (属性に記録された値での) (total_payに対する) 変微分係数を取得．\n",
    "# これはノードから左方向(逆方向)に吐き出されるイメージ．\n",
    "# ここで，実際に中身で計算されているのは\n",
    "# d(apple_sum) / d(apple_price)  と  d(apple_sum) / d(apple_num)  だけで，\n",
    "# この２つに右からの入力で渡した dapple_sum == d(total_pay) / d(apple_sum) を掛けて，\n",
    "# d(apple_sum) / d(apple_price) × d(total_pay) / d(apple_sum)\n",
    "#  = d(apple_price) / d(total_pay) という感じで．\n",
    "# 連鎖律(合成関数微分，chain rule) を活用して楽に算出している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    \"\"\"足し算レイヤ\n",
    "    \n",
    "    f(x, y) = x + y の変換に対応するレイヤ\n",
    "    \n",
    "    Attributes:\n",
    "      x: 足し算レイヤに左から入力された x つまり f(x,y) = x + y の x．\n",
    "      y: 同じく． forward() では足し算に使われ， backward() では勾配を求めたい座標となる．\n",
    "          NN では繰り返し更新され続けるパラ値が状態として記録される．\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \n",
    "        初期化．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\" forward propagation\n",
    "        \n",
    "        足し算を実行して右方向に出力．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation\n",
    "        \n",
    "        アトリビュートに保存された座標での足し算関数に対する勾配を算出し，\n",
    "        それを dout に chain rule で掛け合わせ， 左へ出力．\n",
    "        \n",
    "        Note:\n",
    "          掛け算レイヤの時と違って，偏微分係数は座標によらず一定である．\n",
    "          なので， self.x と self.y に forward 時の入力を記録しておく必要も，今回は無い．\n",
    "          ただまあ，一般的には座標によって勾配違うので，ちゃんと self.x,y に記録することにする．\n",
    "        \n",
    "        \"\"\"\n",
    "        dx = 1 * dout  # d(final) / d(x)  =  d(out) / d(x) * d(final) / d(out)\n",
    "        dy = 1 * dout  # d(final) / d(y)  =  d(out) / d(y) * d(final) / d(out)\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 450\n",
      "650\n",
      "715.0000000000001\n",
      "1.1 650\n",
      "1.1 1.1\n",
      "110.00000000000001 2.2\n",
      "165.0 3.3000000000000003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# makeup Layers\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_fruit_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "\n",
    "# forward propagation\n",
    "\n",
    "apple_num = 2\n",
    "apple_price = 100\n",
    "orange_num = 3\n",
    "orange_price = 150\n",
    "tax = 1.1\n",
    "\n",
    "apple_sum = mul_apple_layer.forward(x=apple_num, y=apple_price)\n",
    "orange_sum = mul_orange_layer.forward(x=orange_num, y=orange_price)\n",
    "fruit_sum = add_fruit_layer.forward(x=apple_sum, y=orange_sum)\n",
    "total_pay = mul_tax_layer.forward(x=fruit_sum, y=tax)\n",
    "print(apple_sum, orange_sum)\n",
    "print(fruit_sum)\n",
    "print(total_pay)\n",
    "\n",
    "\n",
    "# backward propagation\n",
    "\n",
    "d_total_pay = 1    #  d(total_pay) / d(total_pay)\n",
    "\n",
    "d_fruit_sum, d_tax = mul_tax_layer.backward(dout=d_total_pay)\n",
    "print( d_fruit_sum, d_tax)\n",
    "#  d(total_pay) / d(fruit_sum)  と  d(total_pay) / d(tax)\n",
    "\n",
    "d_apple_sum, d_orange_sum = add_fruit_layer.backward(dout=d_fruit_sum)\n",
    "print( d_apple_sum, d_orange_sum )\n",
    "#  d(total_pay) / d(apple_sum)\n",
    "#    =  d(total_pay) / d(fruit_sum)  *  d(fruit_sum) / d(apple_sum)\n",
    "#  d(total_pay) / d(orange_sum)\n",
    "#    = d(total_pay) / d(fruit_sum)  * d(fruit_sum) / d(orange_sum)\n",
    "\n",
    "d_apple_num, d_apple_price = mul_apple_layer.backward(dout=d_apple_sum)\n",
    "print( d_apple_num, d_apple_price )\n",
    "#  d(total_pay) / d(apple_num)\n",
    "#    = d(total_pay) / d(apple_sum)  *  d(apple_sum) / d(apple_num)\n",
    "# d(total_pay) / d(apple_price)\n",
    "#    = d(total_pay) / d(apple_sum) * d(apple_sum) / d(apple_price)\n",
    "\n",
    "d_orange_num, d_orange_price = mul_orange_layer.backward(dout=d_orange_sum)\n",
    "print( d_orange_num, d_orange_price )\n",
    "#  d(total_pay) / d(orange_num)\n",
    "#    = d(total_pay) / d(orange_sum)  *  d(orange_sum) / d(orange_num)\n",
    "#  d(total_pay) / d(orange_price)\n",
    "#    = d(total_pay) / d(orange_sum)  *  d(orange_sum) / d(orange_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 図 5-17\n",
    "\n",
    "計算グラフでは変数が矢印で表現されているが，\n",
    "\n",
    "- forward 矢印にはその変数の値\n",
    "- backward 矢印にはその変数の値における最終出力の勾配(偏微分係数)\n",
    "\n",
    "が対応している．そう捉えると分かりやすい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5. 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> それでは、計算グラフの考え方をニューラルネットワークに適用したいと思います。ここでは、ニューラルネットワークを構成する「層(レイヤ)」をひとつのクラスとして実装することにします。\n",
    "\n",
    "ここでの層(レイヤ)は NN の変換(アフィン, 活性化など)のことを指してる．最初の方(図 2-13, 図3-1とか)ではニューロン群を層と見做していたが，今は違う．まあどっちの見方もメリットがある (前者は back/forward propagation の実装がしやすく，後者は神経ネットワーク的な感じで分かりやすい)ので，どっちもできるように．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.1.  ReLU レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 活性化関数クラスの実装\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\" ReUL layer\n",
    "    \n",
    "    テキストの実装から自分好みに変えてる\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" forward propagation\n",
    "        \n",
    "        activate vector x by ReLU function.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        out = np.where(x >= 0, x, 0)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation\n",
    "        \n",
    "        calcurate gradient by using chain rule.\n",
    "        d(loss) / d(x)  =  d(loss) / d(out)  *  d(out) / d(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        dx = np.where(self.x >= 0, 1, 0)\n",
    "        return dout * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 0.  0.5 0. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 0.9 0.0 0.5 0.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 1 0 1 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ↑の ReLU クラス実装の試行錯誤\n",
    "\n",
    "x = np.array([0.9, -0.4, 0.5, -0.3])\n",
    "x >= 0\n",
    "print( np.where(x >= 0, x, 0) )\n",
    "# https://note.nkmk.me/python-numpy-where/\n",
    "\n",
    "%R x <- c(0.9, - 0.4, 0.5, - 0.3)\n",
    "%R print( ifelse(x >= 0, x, 0) )\n",
    "\n",
    "\n",
    "print( np.where(x >= 0, 1, 0) )\n",
    "%R print( ifelse(x >=0, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2.  Sigmoid レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dy}{dx}\n",
    "&= \\frac{d}{dx} \\left[ \\frac{1}{1 + \\exp(-x)} \\right] \\\\\n",
    "&= -1 * \\left(1 + \\exp(-x) \\right)^{-2}\n",
    " \\frac{d}{dx} [1+\\exp(-x)]  \\\\\n",
    "&= \\left(1 + \\exp(-x) \\right)^{-2} \\exp(-x)  \\\\\n",
    "&= \\frac{1}{1 + \\exp(-x)} \\frac{\\exp(-x)}{1 + \\exp(-x)} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-x)} \\left( 1 - \\frac{1}{1 + \\exp(-x)} \\right) \\\\\n",
    "&= y~(1-y)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid レイヤの実装\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\" sigmoid layer\n",
    "    \n",
    "    テキストの実装から自分好みに少し変えてる．\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize \n",
    "        \"\"\"\n",
    "        x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" forward propagation\n",
    "        \n",
    "        勾配 d(out) / d(x) を計算する際，↑で確認したように，\n",
    "        シグモイドの入力 x を使うより出力 y を使った方が簡単に済む．\n",
    "        なので， self.x = x として x を記録しておくのではなく，　out　の方を記録しておいて，\n",
    "        それを backward() の時に使うようにする．\n",
    "        \n",
    "        \"\"\"\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation \n",
    "        \n",
    "        ↑で確認したように解析的に勾配 d(out) / d(x) が求まる．\n",
    "        chain rule を使って\n",
    "        d(loss) / d(x)  =  d(loss) / d(out)  *  d(out) / d(x)\n",
    "        を算出して返す．\n",
    "        \n",
    "        \"\"\"\n",
    "        dx = self.out * (1.0 - self.out)\n",
    "        return dout * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
