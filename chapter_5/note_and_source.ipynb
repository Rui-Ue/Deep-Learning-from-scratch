{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "# 本書公式 GitHub (https://github.com/oreilly-japan/deep-learning-from-scratch) で提供されてるモジュール．\n",
    "# カレントディレクトリに dataset をダウンロードして置いておく，\n",
    "# あるいは普通にリモートリポジトリごと\n",
    "# $ git clone https://github.com/oreilly-japan/deep-learning-from-scratch.git\n",
    "# でローカルに git clone しておく．\n",
    "\n",
    "# from common.math_functions import *\n",
    "\n",
    "# %load_ext rpy2.ipython\n",
    "# rpy2 で R も使いたい場合↑\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 誤差逆伝搬法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p123] なお、誤差逆伝播法を計算グラフによって説明するアイデアは、Andrej Karpathy のブログ [4] 、また、彼と Fei-Fei Li 教授らによって行われたスタンフォード大学の ディープラーニングの授業「CS231n」[5] を参考にしています。\n",
    "\n",
    "[Andrej Karpathy](https://cs.stanford.edu/people/karpathy/) さんは Tesla の CV 系 AI 研究者で，ブログは[ここ](http://karpathy.github.io/neuralnets/)．PhD の指導教官が Stanford の [Fei-Fei Li](https://profiles.stanford.edu/fei-fei-li?tab=bio) さんで，この方は GoogleCloud で Chief Scientist してたらしい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1. 計算グラフ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p123] 計算グラフとは、計算の過程をグラフによって表したものです。ここで言うグラフ とは、データ構造としてのグラフであり、複数のノードとエッジによって表現されます\n",
    " \n",
    "計算グラフは「演算のフローを可視化したグラフ」という感じ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p124] 計算グラフはノードと矢印によって計算の過程を表します。ノードは○で表記し、 ○の中に演算の内容を書きます。また、計算の途中結果を矢印の上部に書くことで、 ノードごとの計算結果が左から右へ伝わるように表します。\n",
    "\n",
    "前述のニューロンのグラフ (図3-18とか) ではノードが「変数(入力,特徴量,出力)」で矢印が「変換(写像)」というイメージだったが，ここの計算グラフ(図5-3とか)では逆でノードが「演算(変換,写像)」で矢印が「変数(入力,特徴量,出力)」というイメージ．混同しないように．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p126] ここまで見てきたように、計算グラフを使って問題を解くには、\n",
    "1. 計算グラフを構築する\n",
    "2. 計算グラフ上で計算を左から右へ進める\n",
    ">\n",
    ">という流れで行います。ここで 2 番目の「計算を左から右へ進める」というステップは、順方向の伝播、略して、**順伝播**(forward propagation)と言います。順伝播は、計算グラフの出発点から終着点への伝播です。順伝播という名前があるのであれば、逆方向――図で言うと、右から左方向へ――の伝播も考えることができるでしょう。実際、それを**逆伝播**(backward propagation)と言います。逆伝播は、この先、 微分を計算するにあたって重要な働きをします。\n",
    "\n",
    "そもそも計算グラフは「演算の過程を左スタート右ゴールで可視化したもの」なので，左から右へ計算を進める過程を「順伝搬」と呼ぶのは自然．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p127] たとえば、リンゴとそれ以外の買い物を合計する計算―― 4,000 + 200 → 4,200――は、4,000 という数字がどのように計算されてきたかということについては考えずに、ただ 2 つの数字を足せばよいということを意味します。言い換えれば、各ノードの計算で行うべきことは、自分に関係する計算――この例では、入力された 2 つの数字の足し算――だけであり、 全体のことについては何も考えなくてよいのです。\n",
    "このように、計算グラフでは、局所的な計算に集中することができます。たとえ全体の計算がどんなに複雑であったとしても、各ステップでやることは、対象とするノードの「局所的な計算」なのです。局所的な計算は単純ですが、その結果を伝達することで、全体を構成する複雑な計算の結果が得られます。... 計算グラフも、複雑な計算を「単純で局所的な計算」に分割して、流れ 作業を行うように、計算の結果を次のノードへと伝達していきます\n",
    "\n",
    "計算グラフのこのメリットは，関数型プログラミングパラダイムのメリットと似ている．大きい処理を細かい関数に分けることで，各関数の入出力の仕様だけに注意を払って実装とか単体テストができるようになり，気持ちが楽でミスりづらいしデバッグしやすい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p128] ここでは、リンゴの値段に関する微分だけを求めましたが、「消費税に関する支払金額の微分」や「リンゴの個数に関する支払金額の微分」も同様の手順で求めることができます。そして、その際には、途中まで求めた微分(途中まで流れた微分)の結果を共有することができ、効率良く複数の微分を計算することができるのです。\n",
    "\n",
    "ここが back propagation の肝だと思う．途中までの微分の結果を共有できるので，無駄が削られ計算コストが下がる．\n",
    "\n",
    "電気信号回路(ニューロン)でふわっとしたイメージすると...  \n",
    "これまでのナイーブな勾配算出法は，\n",
    "- 第1層の各ニューロンから出る信号をちょっと強めて，最後の信号 (出力層から出た信号をロス関数に入れて出る信号) がどのくらい変化するか見る．それで第1層の偏微分係数ゲット．\n",
    "- 第2層の各ニューロンから出る信号をちょっと強めて，最後の信号がどのくらい変化するか見る．それで第2層の偏微分係数ゲット．\n",
    "- ...\n",
    "\n",
    "という感じ．ここでポイントなのが，例えば第 $n-1$ 中間層と第 $n$層(出力層)の間には，何度も(全パラメータ数分)だけ信号が流れている．  \n",
    "一方，誤差逆伝搬では，\n",
    "\n",
    "- 第 $n-1$ 層の各ニューロンから出る信号をちょっと強めて，最後の信号がどのくらい変化するか見る．それで第 $n-1$ 層の偏微分係数ゲット．そしてその情報を記録してとっておく．\n",
    "- 第 $n-2$ 層の各ニューロンから出る信号をちょっと強めて，次の第 $n-1$ 層に入る信号がどのくらい変化するかを見る．それを先ほど記録した $n-1$ 層の偏微分係数に掛け合わせて，第 $n-2$ 層の変微分係数を取得．この情報も記録しておく．\n",
    "- ...\n",
    "\n",
    "という感じ．連鎖律を使ったことで，例えば先ほどの方法だと何度も信号が流されていた $n-1$ 層と $n$ 層の間に，１回だけしか信号が流れていない．無駄を削ぎ落とせている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もっと正確に説明する．重みパラメータはニューロン間の繋がりの強さ(通路,チャネルの太さ)で，最初の input 信号が生の説明変数値，最後の output 信号がロス関数値，みたいなイメージを持って．\n",
    "なので目標は「最初に input される信号 (ロス算出対象データ) はもう決まっていて固定で，各チャネルの太さを若干変化させた時に最後に output される信号がどう変化するか調べる」こと．\n",
    "\n",
    "これまでのナイーブな勾配算出法は，どの層のチャネルについて調べる時も全く同じことをする．「そのチャネルをちょっと細くして (-h ずらして) 最初から入力信号をフルでforwardで流して，次はチャネルとちょっと太くして (+hずらして) 同じことをして，変化率を計算」という繰り返し作業を，全チャネルに対してやる．確実だが賢くなく効率が悪い．\n",
    "\n",
    "一方， back propagation ではさっきとは違って，層ごとに取り扱い方を変えたり工夫して賢く調べていく．\n",
    "\n",
    "- はじめに１回だけフルで forward 方向に信号を流し，その時に各チャネルに流れた(各ノードから出た)信号の強さとかを記録して保持しておく．\n",
    "- そのあとまず，第$n$層のニューロンから先ほどより少しだけ変化させた信号を出してロス関数に渡して，ロス値がどう変化するかを記録(1)．\n",
    "- 次に，第 $n$ 層のチャネルの太さをちょっと変えて第$n-1$層のニューロンから最初のforward時と同じ強さの信号を流して第$n$層に伝わる信号の変化を記録する．これを(1)と掛け合わせることで第 $n$ 層のチャネルの太さの変化がロス値にどういう変化をもたらすか見積もれるはず(chain rule,直感的に考えても納得)なので算出，目的の一部達成．\n",
    "- 後のために，今度は $n$ 層のチャネルの太さはそのまま固定して第$n-1$層のニューロンから最初のforward時から少し変化させた強さの信号を流して第$n$層に伝わる信号の変化を記録する(2)\n",
    "- 次に，第 $n-1$ 層のチャネルの太さをちょっと変えて第$n-2$層のニューロンから最初のforward時と同じ強さの信号を流して第$n-1$層に伝わる信号の変化を記録する．これを(1)(2)と掛け合わせることで第 $n-1$ 層のチャネルの太さの変化がロス値にどういう変化をもたらすか見積もれるはず(chain rule,直感的にも納得)なので算出．目的の一部達成．\n",
    "- 後のために，今度は $n-1$ 層のチャネルの太さはそのまま固定して第$n-2$層のニューロンから最初のforward時から少し変化させた強さの信号を流して第$n-1$層に伝わる信号の変化を記録する(3)．\n",
    "- 次に，第 $n-2$ 層のチャネルの太さをちょっと変えて第$n-3$層のニューロンから最初のforward時と同じ強さの信号を流して第$n-2$層に伝わる信号の変化を記録する．これを(1)(2)(3)と掛け合わせることで第 $n-2$ 層のチャネルの太さの変化がロス値にどういう変化をもたらすか見積もれるはず(chain rule,直感的にも納得)なので算出．目的の一部達成．\n",
    "- 後のために，今度は $n-1$ 層のチャネルの太さはそのまま固定して第$n-2$層のニューロンから最初のforward時から少し変化させた強さの信号を流して第$n-1$層に伝わる信号の変化を記録する(4)．\n",
    "- ...\n",
    "\n",
    "という感じ．明らかにナイーブな方法より，チャネルを通る信号の総量が少ない．つまりやるべき行列計算の量が少ない．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2. 連鎖律"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフのノードは\n",
    "\n",
    "- 左から入力された信号に対しては，その値をノード関数で変換して結果を右に出力する\n",
    "- 右から入力された信号に対しては，それにノード関数の微分を掛け合わせて左に出力する\n",
    "\n",
    "という挙動を示す(ことにしている)．そうすると色々便利だから．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p131] 図 5-7 で注目すべきは、一番左の逆伝播の結果です。これは、連鎖律より、\n",
    "...「x に関する z の微分」に対応します。つまり、逆伝播が行っていることは、連鎖律の原理から構成されているのです。\n",
    "\n",
    "もはや計算グラフは「合成関数の微分を分かりやすく求めるためのツール」とも捉えられそう．普通に微積のテストとかでも役立ちそう．\n",
    "\n",
    "少し違っていて注意しないといけないのは， SGD においては「合成関数の導関数」ではなく 「合成関数の微分係数」という１点の値を求めようとしてる，ということ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3. 逆伝搬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p132] なお、この例では上流から伝わった微分の値を $\\frac{\\partial L}{\\partial z}$ としましたが、これは図5-10に示すように、最終的に $L$ という値を出力する大きな計算グラフを想定しているため\n",
    "\n",
    "NN の SGD とかでは「各パラメータをちょっと動かした時にロス $L$ がどのくらい変化するか」という勾配を求めようとするので，計算グラフの最後には必ず，ロス $L$ を出力するロス関数ノードがあるはず．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4. 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p137] 次節では、ニューラルネットワークを構成する「層(レイヤ)」をひとつのクラスで実装することにします。ここで言う「レイヤ」とは、ニューラルネットワー クにおける機能の単位です。たとえば、シグモイド関数のための Sigmoid や、 行列の積のための Affine など、レイヤ単位で実装を行います。そのため、こ こでも「レイヤ」という単位で、乗算ノードと加算ノードを実装します。\n",
    "\n",
    "次節以降 (例えば図5-28) では，当初の俺が思ってた通り変換 (変数じゃなくて) を層 (レイヤー) とみなしている．このグラフでは変換がノードで表されててノード群を層と見るのは視覚的に直感的．この捉え方なら $n$ 層ニューラルネットワークっていう用語も「$n$ 回(層)の変換を行う NN」と解釈できて，分かりやすい．\n",
    "\n",
    "そう考えると，前述の図3-18みたいな「変数がノードで変換が矢印」のグラフより，後述の図5-14~5-30みたいな「変数が矢印で変換がノード」のグラフの方が「層(レイヤー)」のイメージがつきやすくて良いかも．まあ，どっちの見方もできるようにしとくのが良い．\n",
    "\n",
    "計算グラフは NN を実装する上での良い設計図になっているってことか．ノード(あるいは複数ノードをまとめたレイヤー)を class で実装して，そのインスタンスを組み合わせることで NN 全体を実装できるから．Pytorch もそんな感じだった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗算レイヤの実装 (5.4.1 / p137)\n",
    "\n",
    "class MulLayer:\n",
    "    \"\"\"掛け算レイヤを表すクラス\n",
    "    \n",
    "    f(x,y) = xy という変換に対応するクラス．\n",
    "    計算グラフのノードをそのまま実装するイメージ．\n",
    "    \n",
    "    Attributes:\n",
    "      x: 掛け算関数f(x,y) = x*y の x．\n",
    "      y: 同じく． のちの NN では，更新が繰り返されるパラメータ値が状態として保存されている．\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \n",
    "        アトリビュートの初期化を __init__() ではなく forward() メソッドでやる理由\n",
    "        ニューラルネットで逆伝搬する時のフローをイメージしてる．\n",
    "        １回だけ予測値とロスを算出して(forward して)，\n",
    "        そこから戻りながら勾配を計算していく(backward する)というフローが採られるので，\n",
    "        それを想定して最初の forward 時にアトリビュートの初期化が起こるように書いてある．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"掛け算レイヤの forward 処理\n",
    "        \n",
    "        つまり，ただ掛け算を実行して結果を出力する．\n",
    "        ここでアトリビュート(状態)の初期化や更新が行われる理由は，↑を参照．\n",
    "        \n",
    "        Returns:\n",
    "          - self.x * self.y\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"掛け算レイヤの backward 処理\n",
    "        \n",
    "        つまり，状態 x,y に保存されている値での掛け算関数の勾配を計算する．\n",
    "        つまり，座標(self.x, self.y)における掛け算関数の勾配を計算する．\n",
    "        さらに(計算グラフ上の右から)入力された値 dout に算出された勾配を掛け合わせる．\n",
    "        そしてそれを出力する．\n",
    "        \n",
    "        Returns:\n",
    "          - self.x における f(x,y)=x*y の偏微分係数(勾配)\n",
    "          - self.y における f(x,y)=x*y の偏微分係数(勾配)        \n",
    "        \n",
    "        \"\"\"\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100 2\n",
      "220.00000000000003\n",
      "200 1.1\n",
      "1.1 200\n",
      "2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "# ↑で定義した掛け算レイヤの動作確認\n",
    "\n",
    "# get layer\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "# 各レイヤ(計算グラフのノード)ごとのオブジェクトを作成．\n",
    "\n",
    "\n",
    "# forward\n",
    "\n",
    "apple_price = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "apple_sum = mul_apple_layer.forward(x=apple_price, y=apple_num)\n",
    "print(apple_sum)  # forward 伝搬での出力\n",
    "print(mul_apple_layer.x, mul_apple_layer.y)  # 状態に forward 伝搬での入力が記録されている\n",
    "\n",
    "total_pay = mul_tax_layer.forward(x=apple_sum, y=tax)\n",
    "print(total_pay)  # forward 伝搬での出力\n",
    "print(mul_tax_layer.x, mul_tax_layer.y)  # 状態に forward 伝搬での入力が記録されてる\n",
    "\n",
    "\n",
    "# backward\n",
    "\n",
    "dtotal_pay = 1\n",
    "# いまの合成関数全体の最終出力は total_pay で，\n",
    "# backward に　chain rule で勾配を求めていく都合上，\n",
    "# 最初に d(total_pay) / d(total_pay) という自分自身による微分係数(常に１)を作っとく必要がある．\n",
    "\n",
    "dapple_sum, dtax = mul_tax_layer.backward(dtotal_pay)\n",
    "print(dapple_sum, dtax)\n",
    "# apple_sum, tax の(forward 時に属性に記録した値での) (total_pay に対する) 偏微分係数を算出．\n",
    "# これはノードから左方向(逆方向)に吐き出されるイメージ．\n",
    "# d(total_pay) / d(apple_sum)  と  d(total_pay) / d(tax)　という感じ．\n",
    "# 最終出力(目的関数値) total_pay に直接影響してる変数なので， chain rule は活用してない．\n",
    "\n",
    "dapple_price, dapple_num = mul_apple_layer.backward(dapple_sum)\n",
    "print(dapple_price, dapple_num)\n",
    "# apple_price, apple_num の (属性に記録された値での) (total_payに対する) 変微分係数を取得．\n",
    "# これはノードから左方向(逆方向)に吐き出されるイメージ．\n",
    "# ここで，実際に中身で計算されているのは\n",
    "# d(apple_sum) / d(apple_price)  と  d(apple_sum) / d(apple_num)  だけで，\n",
    "# この２つに右からの入力で渡した dapple_sum == d(total_pay) / d(apple_sum) を掛けて，\n",
    "# d(apple_sum) / d(apple_price) × d(total_pay) / d(apple_sum)\n",
    "#  = d(apple_price) / d(total_pay) という感じで．\n",
    "# 連鎖律(合成関数微分，chain rule) を活用して楽に算出している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 足し算レイヤの実装\n",
    "\n",
    "\n",
    "class AddLayer:\n",
    "    \"\"\"足し算レイヤ\n",
    "    \n",
    "    f(x, y) = x + y の変換に対応するレイヤ\n",
    "    \n",
    "    Attributes:\n",
    "      x: 足し算レイヤに左から入力された x つまり f(x,y) = x + y の x．\n",
    "      y: 同じく． forward() では足し算に使われ， backward() では勾配を求めたい座標となる．\n",
    "          NN では繰り返し更新され続けるパラ値が状態として記録される．\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \n",
    "        初期化．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\" forward propagation\n",
    "        \n",
    "        足し算を実行して右方向に出力．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation\n",
    "        \n",
    "        アトリビュートに保存された座標での足し算関数に対する勾配を算出し，\n",
    "        それを dout に chain rule で掛け合わせ， 左へ出力．\n",
    "        \n",
    "        Note:\n",
    "          掛け算レイヤの時と違って，偏微分係数は座標によらず一定である．\n",
    "          なので， self.x と self.y に forward 時の入力を記録しておく必要も，今回は無い．\n",
    "          ただまあ，一般的には座標によって勾配違うので，ちゃんと self.x,y に記録することにする．\n",
    "        \n",
    "        \"\"\"\n",
    "        dx = 1 * dout  # d(final) / d(x)  =  d(out) / d(x) * d(final) / d(out)\n",
    "        dy = 1 * dout  # d(final) / d(y)  =  d(out) / d(y) * d(final) / d(out)\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 450\n",
      "650\n",
      "715.0000000000001\n",
      "1.1 650\n",
      "1.1 1.1\n",
      "110.00000000000001 2.2\n",
      "165.0 3.3000000000000003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# makeup Layers\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_fruit_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "\n",
    "# forward propagation\n",
    "\n",
    "apple_num = 2\n",
    "apple_price = 100\n",
    "orange_num = 3\n",
    "orange_price = 150\n",
    "tax = 1.1\n",
    "\n",
    "apple_sum = mul_apple_layer.forward(x=apple_num, y=apple_price)\n",
    "orange_sum = mul_orange_layer.forward(x=orange_num, y=orange_price)\n",
    "fruit_sum = add_fruit_layer.forward(x=apple_sum, y=orange_sum)\n",
    "total_pay = mul_tax_layer.forward(x=fruit_sum, y=tax)\n",
    "print(apple_sum, orange_sum)\n",
    "print(fruit_sum)\n",
    "print(total_pay)\n",
    "\n",
    "\n",
    "# backward propagation\n",
    "\n",
    "d_total_pay = 1    #  d(total_pay) / d(total_pay)\n",
    "\n",
    "d_fruit_sum, d_tax = mul_tax_layer.backward(dout=d_total_pay)\n",
    "print( d_fruit_sum, d_tax)\n",
    "#  d(total_pay) / d(fruit_sum)  と  d(total_pay) / d(tax)\n",
    "\n",
    "d_apple_sum, d_orange_sum = add_fruit_layer.backward(dout=d_fruit_sum)\n",
    "print( d_apple_sum, d_orange_sum )\n",
    "#  d(total_pay) / d(apple_sum)\n",
    "#    =  d(total_pay) / d(fruit_sum)  *  d(fruit_sum) / d(apple_sum)\n",
    "#  d(total_pay) / d(orange_sum)\n",
    "#    = d(total_pay) / d(fruit_sum)  * d(fruit_sum) / d(orange_sum)\n",
    "\n",
    "d_apple_num, d_apple_price = mul_apple_layer.backward(dout=d_apple_sum)\n",
    "print( d_apple_num, d_apple_price )\n",
    "#  d(total_pay) / d(apple_num)\n",
    "#    = d(total_pay) / d(apple_sum)  *  d(apple_sum) / d(apple_num)\n",
    "# d(total_pay) / d(apple_price)\n",
    "#    = d(total_pay) / d(apple_sum) * d(apple_sum) / d(apple_price)\n",
    "\n",
    "d_orange_num, d_orange_price = mul_orange_layer.backward(dout=d_orange_sum)\n",
    "print( d_orange_num, d_orange_price )\n",
    "#  d(total_pay) / d(orange_num)\n",
    "#    = d(total_pay) / d(orange_sum)  *  d(orange_sum) / d(orange_num)\n",
    "#  d(total_pay) / d(orange_price)\n",
    "#    = d(total_pay) / d(orange_sum)  *  d(orange_sum) / d(orange_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 図 5-17\n",
    "\n",
    "計算グラフでは変数が矢印で表現されているが，\n",
    "\n",
    "- forward 矢印にはその変数の値\n",
    "- backward 矢印にはその変数の値における最終出力の勾配(偏微分係数)\n",
    "\n",
    "が対応している．そう捉えると分かりやすい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5. 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> それでは、計算グラフの考え方をニューラルネットワークに適用したいと思います。ここでは、ニューラルネットワークを構成する「層(レイヤ)」をひとつのクラスとして実装することにします。\n",
    "\n",
    "ここでの層(レイヤ)は NN の変換(アフィン, 活性化など)のことを指してる．最初の方(図 2-13, 図3-1とか)ではニューロン群を層と見做していたが，今は違う．まあどっちの見方もメリットがある (前者は back/forward propagation の実装がしやすく，後者は神経ネットワーク的な感じで分かりやすい)ので，どっちもできるように．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.1.  ReLU レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 活性化関数クラスの実装\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\" ReUL layer\n",
    "    \n",
    "    テキストの実装から自分好みに変えてる\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \"\"\"\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" forward propagation\n",
    "        \n",
    "        activate vector x by ReLU function.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        out = np.where(x >= 0, x, 0)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation\n",
    "        \n",
    "        calcurate gradient by using chain rule.\n",
    "        d(loss) / d(x)  =  d(loss) / d(out)  *  d(out) / d(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        dx = np.where(self.x >= 0, 1, 0)\n",
    "        return dout * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   2.1  0.  12.1]\n",
      "[ 0 10  0 10]\n"
     ]
    }
   ],
   "source": [
    "# ↑の ReLU クラスの動作確認\n",
    "\n",
    "relu_layer = ReLU()\n",
    "print( relu_layer.forward(x=np.array([-0.4, 2.1, -10, 12.1])) )\n",
    "print( relu_layer.backward(dout=10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 0.  0.5 0. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 0.9 0.0 0.5 0.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 1 0 1 0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ↑の ReLU クラス実装の試行錯誤\n",
    "\n",
    "x = np.array([0.9, -0.4, 0.5, -0.3])\n",
    "x >= 0\n",
    "print( np.where(x >= 0, x, 0) )\n",
    "# https://note.nkmk.me/python-numpy-where/\n",
    "\n",
    "%R x <- c(0.9, - 0.4, 0.5, - 0.3)\n",
    "%R print( ifelse(x >= 0, x, 0) )\n",
    "\n",
    "\n",
    "print( np.where(x >= 0, 1, 0) )\n",
    "%R print( ifelse(x >=0, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2.  Sigmoid レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dy}{dx}\n",
    "&= \\frac{d}{dx} \\left[ \\frac{1}{1 + \\exp(-x)} \\right] \\\\\n",
    "&= -1 * \\left(1 + \\exp(-x) \\right)^{-2}\n",
    " \\frac{d}{dx} [1+\\exp(-x)]  \\\\\n",
    "&= \\left(1 + \\exp(-x) \\right)^{-2} \\exp(-x)  \\\\\n",
    "&= \\frac{1}{1 + \\exp(-x)} \\frac{\\exp(-x)}{1 + \\exp(-x)} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-x)} \\left( 1 - \\frac{1}{1 + \\exp(-x)} \\right) \\\\\n",
    "&= y~(1-y)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid レイヤの実装\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\" sigmoid layer\n",
    "    \n",
    "    テキストの実装から自分好みに少し変えてる．\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize \n",
    "        \"\"\"\n",
    "        x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" forward propagation\n",
    "        \n",
    "        勾配 d(out) / d(x) を計算する際，↑で確認したように，\n",
    "        シグモイドの入力 x を使うより出力 y を使った方が簡単に済む．\n",
    "        なので， self.x = x として x を記録しておくのではなく，　out　の方を記録しておいて，\n",
    "        それを backward() の時に使うようにする．\n",
    "        \n",
    "        \"\"\"\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation \n",
    "        \n",
    "        ↑で確認したように解析的に勾配 d(out) / d(x) が求まる．\n",
    "        chain rule を使って\n",
    "        d(loss) / d(x)  =  d(loss) / d(out)  *  d(out) / d(x)\n",
    "        を算出して返す．\n",
    "        \n",
    "        \"\"\"\n",
    "        dx = self.out * (1.0 - self.out)\n",
    "        return dout * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42555748 0.57444252 0.5        0.11920292 0.88079708]\n",
      "[0.24445831 0.24445831 0.25       0.10499359 0.10499359]\n"
     ]
    }
   ],
   "source": [
    "# ↑の Sigmoid クラスの動作確認\n",
    "\n",
    "sigmoid_layer = Sigmoid()\n",
    "print( sigmoid_layer.forward(np.array([-0.3, +0.3, 0.0, -2.0, +2.0])) )\n",
    "print( sigmoid_layer.backward(dout=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6. Affine / Softmax レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.1. Affine レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p147] ニューラルネットワークの順伝播で行う行列の積は、幾何学の分野では「アフィン変換」と呼ばれます。\n",
    "\n",
    "アフィン変換は線形変換と平行移動を組み合わせたもの．数式で言うと，$x$ のアフィン変換は $A x + b$ と言う感じ． 2次元から2次元とか，3次元から3次元のアフィン変換は幾何学的な解釈が可能で，ベクトル(図形)の回転＆拡大縮小＆平行移動みたいなイメージ．PRML のガウス分布のとこでやった「基底ベクトルを列とした行列の逆行列をかけると，その規定ベクトルの座標軸での成分に変換できる」とかも思い出して．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p148] 式 (5.13) が導かれる過程はここでは省略します\n",
    "\n",
    "ここやってみる．行列積レイヤの逆伝搬を導く，つまり $W, x$ を変数としたロス関数値 $l$ の勾配を導く．ここで $l$ は $a_1, a_2, \\ldots, a_n$ の関数．ポイントとしては，変数をノードとした NN グラフを想像して，全ての経路(パス)についての偏微分積を足し合わせるイメージで，多変数の合成関数の微分を丁寧にやる．院試で使ったテキスト参照．また NN でのこれまでのノーテーションを意識して $y$ の代わりに $a$ を使っている．$L$ の代わりに値 $l$ を使っているのは $L$ はクロスエントロピー関数とか二乗誤差関数とかを表すのに使いたいから．パワポも参照．また，$P$ 次元から $Q$ 次元へのアフィン変換について考えている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず， $x$ を変数としたロス関数値 $l$ の勾配ベクトルは，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial x} = \\left( \\frac{\\partial l}{\\partial x_1}, \n",
    "\\frac{\\partial l}{\\partial x_2}, \\ldots, \\frac{\\partial l}{\\partial x_p} \\right)\n",
    "$$\n",
    "\n",
    "であり，この第 $i$ 成分を求めていくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial x_i}\n",
    "&= \\frac{\\partial l}{\\partial a_1} \\frac{\\partial a_1}{\\partial x_i}\n",
    "+ \\frac{\\partial l}{\\partial a_2} \\frac{\\partial a_2}{\\partial x_i}\n",
    "+ \\cdots\n",
    "+ \\frac{\\partial l}{\\partial a_q} \\frac{\\partial a_q}{\\partial x_i} \\\\\n",
    "&= \\left(\\frac{\\partial l}{\\partial a_1}, \\frac{\\partial l}{\\partial a_2}, \\ldots\n",
    "\\frac{\\partial l}{\\partial a_q} \\right)\n",
    "\\left(\\frac{\\partial a_1}{\\partial x_i}, \\frac{\\partial a_2}{\\partial x_i}, \\ldots\n",
    "\\frac{\\partial a_q}{\\partial x_i} \\right)^T \\\\\n",
    "&= \\frac{\\partial l}{\\partial a}\n",
    "\\left(\\frac{\\partial \\sum_{j=1}^p x_j w_{j1} + b_1}{\\partial x_i},\n",
    "\\frac{\\partial \\sum_{j=1}^p x_j w_{j2} + b_2}{\\partial x_i}, \\ldots\n",
    "\\frac{\\partial \\sum_{j=1}^p x_j w_{jq} + b_q}{\\partial x_i} \\right)^T \\\\\n",
    "&= \\frac{\\partial l}{\\partial y}\n",
    "\\left( w_{i1}, w_{i2}, \\ldots, w_{iq} \\right)^T \\\\\n",
    "&= \\sum_{j=1}^q \\frac{\\partial l}{\\partial a_j} w_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるので，この結果を全要素に適用してあげて，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial x}\n",
    "&= \\left( \\frac{\\partial l}{\\partial x_1}, \n",
    "\\frac{\\partial l}{\\partial x_2}, \\ldots, \\frac{\\partial l}{\\partial x_p} \\right) \\\\\n",
    "&= \\left( \\sum_{j=1}^n \\frac{\\partial L}{\\partial a_j} w_{1j}, \n",
    "\\sum_{j=1}^q \\frac{\\partial l}{\\partial a_j} w_{2j},\n",
    "\\ldots,\n",
    "\\sum_{j=1}^q \\frac{\\partial l}{\\partial a_j} w_{mj}\n",
    "\\right) \\\\\n",
    "&= \\left(\\frac{\\partial l}{\\partial a_1}, \\frac{\\partial l}{\\partial a_2}, \\ldots\n",
    "\\frac{\\partial l}{\\partial a_q} \\right)  W^T \\\\\n",
    "&= \\frac{\\partial l}{\\partial a} W^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，(5.13) の１つめの式を導出できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，$W$ を変数としたロス値 $l$ の勾配行列は，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial W} = \\left\\{ \\frac{\\partial l}{\\partial w_{ij}} \\right\\}\n",
    "$$\n",
    "\n",
    "であり，この $(i,j)$ 成分を求めていくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial w_{ij}}\n",
    "&= \\frac{\\partial l}{\\partial a_j} \\frac{\\partial a_j}{\\partial w_{ij}}\\\\\n",
    "&= \\frac{\\partial l}{\\partial a_j}\n",
    "\\frac{\\partial \\sum_{k=1}^p x_k w_{kj} + b_j}{\\partial w_{ij}}\\\\\n",
    "&= \\frac{\\partial l}{\\partial a_j} x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるので，この結果を全要素に適用してあげて，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial W}\n",
    "&= \\left\\{ \\frac{\\partial l}{\\partial w_{ij}} \\right\\} \\\\\n",
    "&= \\left\\{ \\frac{\\partial l}{\\partial a_j} x_i \\right\\} \\\\\n",
    "&= (x_1, x_2, \\ldots, x_p)^T\n",
    "\\left( \\frac{\\partial l}{\\partial a_1}, \\frac{\\partial l}{\\partial a_1}, \\ldots, \n",
    "\\frac{\\partial l}{\\partial a_q} \\right) \\\\\n",
    "&= x^T \\frac{\\partial l}{\\partial a}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，(5.13) の2つめの式も導出できた．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ついでに，自明だからテキストには書かれていないが，ロス関数 $L$ のバイアス $b$ についての勾配ベクトルも求めておくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial b_i}\n",
    "&= \\frac{\\partial l}{\\partial a_i} \\frac{\\partial a_i}{\\partial b_i} \\\\\n",
    "&= \\frac{\\partial l}{\\partial a_i}\n",
    "\\frac{\\sum_{j=1}^p x_{j} w_{ji} + b_i}{\\partial b_i} \\\\\n",
    "&= \\frac{\\partial l}{\\partial a_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる．\n",
    "\n",
    "ついでに同様に，計算グラフ図 5-12 の $+$ ノードから逆伝搬されている $xW$ についての勾配ベクトルも求めておくと，自明だが，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial (xW)_i}\n",
    "&= \\frac{\\partial l}{\\partial a_i} \\frac{\\partial a_i}{\\partial (xW)_i} \\\\\n",
    "&= \\frac{\\partial l}{\\partial a_i}\n",
    "\\frac{\\sum_{j=1}^p x_{j} w_{ji} + b_i}{\\partial \\sum_{j=1}^p x_{j} w_{ji}} \\\\\n",
    "&= \\frac{\\partial l}{\\partial a_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p149] それでは、式 (5.13) を元に、計算グラフの逆伝播を書いてみましょう。結果は 図5-25 のようになります。\n",
    "\n",
    "多変数の合成関数の微分を使って (5.13) を導けたことで，多変数でも chain rule みたいな形が確認できた．よって，１変数の時と同じように，逆伝搬の際は右から入力された勾配 (後ろの変数の勾配,forwardのときの出力に対応する勾配) に局所的な勾配を掛けることで実装できそう，とわかった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで何層も重ねた場合の back propagation の全体像を説明していく．あとはひたすら図 5-22 とか 5-44 を組み合わせて再起的にやっていくだけ！パワポを参照！\n",
    "\n",
    "本質としては...  \n",
    "「多変数合成関数の微分公式(ライプニッツ則, chain rule)」によって，NNの出力側(右側)から順に局所的な勾配を計算していき，それを掛け合わせたりすることで，効率的にパラメータ勾配を求められる．結局やっていることはライプニッツ則の適用．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.2. バッチ版 Affine レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p150] 伝播の際は、行列の形状に注意すれば、$\\frac{\\partial L}{\\partial X}$ と $\\frac{\\partial L}{\\partial W}$ は前と同じように導出することができます。\n",
    "\n",
    "ここ，ちゃんと導出してみる．ノーテーションとかポイントはさっきと同じ．合成関数微分の際は，院試テキストの図イメージして全てのパスの微分積を足し合わせれば良い．\n",
    "\n",
    "$x_{ij}, a_{ik}$ は，$i$ 番目のケース(合計 $n$ 個)の $j$ 番目の変換前特徴量と $k$ 番目のアフィン変換後特徴量を表すよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずパラメータ更新に使う勾配行列 $\\frac{\\partial l}{\\partial W}$ は，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial W} = \\left\\{ \\frac{\\partial l}{\\partial w_{ij}} \\right\\}\n",
    "$$\n",
    "\n",
    "であり，この $(i,j)$ 成分を求めていくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial w_{ij}}\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{1j}} \\frac{\\partial a_{1j}}{\\partial w_{ij}}\n",
    "+ \\frac{\\partial l}{\\partial a_{2j}} \\frac{\\partial a_{2j}}{\\partial w_{ij}}\n",
    "+ \\cdots\n",
    "+ \\frac{\\partial l}{\\partial a_{nj}} \\frac{\\partial a_{nj}}{\\partial w_{ij}}\\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{1j}}\n",
    "\\frac{\\partial \\sum_{k=1}^p x_{1k} w_{kj} + b_j}{\\partial w_{ij}}\n",
    "+ \\frac{\\partial l}{\\partial a_{2j}}\n",
    "\\frac{\\partial \\sum_{k=1}^p x_{2k} w_{kj} + b_j}{\\partial w_{ij}}\n",
    "+ \\cdots\n",
    "+ \\frac{\\partial l}{\\partial a_{nj}}\n",
    "\\frac{\\partial \\sum_{k=1}^p x_{nk} w_{kj} + b_j}{\\partial w_{ij}} \\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{1j}} x_{1i}\n",
    "+ \\frac{\\partial l}{\\partial a_{2j}} x_{2i}\n",
    "+ \\cdots\n",
    "+ \\frac{\\partial l}{\\partial a_{nj}} x_{ni} \\\\\n",
    "&= (x_{1i}, x_{2i}, \\ldots, x_{ni})\n",
    "\\left(\\frac{\\partial l}{\\partial a_{1j}}, \\frac{\\partial l}{\\partial a_{2j}}, \\dots, \\frac{\\partial l}{\\partial a_{nj}}\\right)^T \\\\\n",
    "&= \\mathrm{inner~product} \\left[\n",
    "(X^T)_{\\mathrm{row}~i} ,~ \\left( \\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{col}~j}\n",
    "\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるので，この結果を全要素に適用してあげて，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial W}\n",
    "&= \\left\\{ \\frac{\\partial l}{\\partial w_{ij}} \\right\\} \\\\\n",
    "&= \\left\\{\\mathrm{inner~product} \\left[\n",
    "(X^T)_{\\mathrm{row}~i}, ~ \\left( \\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{col}~j}\n",
    "\\right] \\right\\} \\\\\n",
    "&= X^T \\frac{\\partial l}{\\partial A}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，図 5-27 の四角 2 を導出できた．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，パラメータ更新に使う勾配ベクトル $\\frac{\\partial l}{\\partial b}$ は，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial b} = \\left( \\frac{\\partial l}{\\partial b_1}, \\frac{\\partial l}{\\partial b_2},\n",
    "\\ldots, \\frac{\\partial l}{\\partial b_q} \\right)\n",
    "$$\n",
    "\n",
    "であり，この第 $i$ 成分を求めていくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial b_i}\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{1i}} \\frac{\\partial a_{1i}}{\\partial b_i} + \n",
    "\\frac{\\partial l}{\\partial a_{2i}} \\frac{\\partial a_{2i}}{\\partial b_i} +\n",
    "\\cdots\n",
    "\\frac{\\partial l}{\\partial a_{ni}} \\frac{\\partial a_{ni}}{\\partial b_i}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{1i}} \\frac{\\sum_{j=1}^p x_{1j} w_{ji} + b_i}{\\partial b_i} + \n",
    "\\frac{\\partial l}{\\partial a_{2i}} \\frac{\\sum_{j=1}^p x_{2j} w_{ji} + b_i}{\\partial b_i} +\n",
    "\\ldots + \n",
    "\\frac{\\partial l}{\\partial a_{ni}} \\frac{\\sum_{j=1}^p x_{nj} w_{ji} + b_i}{\\partial b_i} \\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{1i}} + \\frac{\\partial l}{\\partial a_{2i}} + \\cdots + \\frac{\\partial l}{\\partial a_{ni}} \\\\\n",
    "&= \\mathrm{sum} \\left[ \\left (\\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{col}~i} \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるので，この結果を全要素に適用してあげて，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial b}\n",
    "&=\n",
    "\\left( \\frac{\\partial l}{\\partial b_1}, \\frac{\\partial l}{\\partial b_2},\n",
    "\\ldots, \\frac{\\partial l}{\\partial b_q} \\right) \\\\\n",
    "&=\n",
    "\\left(\n",
    "\\mathrm{sum} \\left[ \\left (\\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{col}~1} \\right], \n",
    "\\mathrm{sum} \\left[ \\left (\\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{col}~2} \\right],  \\ldots, \n",
    "\\mathrm{sum} \\left[ \\left (\\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{col}~q} \\right]\n",
    "\\right) \\\\\n",
    "&=\n",
    "\\mathrm{sum~by~column} \\left[ \\frac{\\partial l}{\\partial A} \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，図 5-27 の四角 3 を導出できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，１つ前の層のパラメータ勾配を chain rule で算出する時に必要となる勾配行列 $\\frac{\\partial l}{\\partial X}$ は，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial X} = \\left\\{ \\frac{\\partial l}{\\partial x_{ij}} \\right\\}\n",
    "$$\n",
    "\n",
    "であり，この $(i,j)$ 成分を求めていくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial x_{ij}}\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial a_{i1}} \\frac{\\partial a_{i1}}{\\partial x_{ij}}\n",
    "+ \\frac{\\partial l}{\\partial a_{i2}} \\frac{\\partial a_{i2}}{\\partial x_{ij}}\n",
    "+ \\cdots\n",
    "+ \\frac{\\partial l}{\\partial a_{iq}} \\frac{\\partial a_{iq}}{\\partial x_{ij}} \\\\\n",
    "&=\n",
    "\\left(\\frac{\\partial l}{\\partial a_{i1}}, \\frac{\\partial l}{\\partial a_{i2}}, \\ldots,\n",
    "\\frac{\\partial l}{\\partial a_{iq}} \\right)\n",
    "\\left(\\frac{\\partial a_{i1}}{\\partial x_{ij}}, \\frac{\\partial a_{i2}}{\\partial x_{ij}}, \\ldots,\n",
    "\\frac{\\partial a_{iq}}{\\partial x_{ij}} \\right)^T \\\\\n",
    "&=\n",
    "\\left(\\frac{\\partial l}{\\partial a_{i1}}, \\frac{\\partial l}{\\partial a_{i2}}, \\ldots,\n",
    "\\frac{\\partial l}{\\partial a_{iq}} \\right)\n",
    "\\left(\\frac{\\partial \\sum_{k=1}^p x_{ik} w_{k1} + b_1}{\\partial x_{ij}},\n",
    "\\frac{\\partial \\sum_{k=1}^p x_{ik} w_{k2} + b_2}{\\partial x_{ij}}, \\ldots,\n",
    "\\frac{\\partial \\sum_{k=1}^p x_{ik} w_{kq} + b_q}{\\partial x_{ij}} \\right)^T \\\\\n",
    "&=\n",
    "\\left(\\frac{\\partial l}{\\partial a_{i1}}, \\frac{\\partial l}{\\partial a_{i2}}, \\ldots,\n",
    "\\frac{\\partial l}{\\partial a_{iq}} \\right)\n",
    "\\left( w_{j1}, w_{j2}, \\ldots, w_{jq} \\right)^T \\\\\n",
    "&=\n",
    "\\mathrm{inner~product}\\left[\n",
    "\\left( \\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{row}~i}, ~\n",
    "\\left( W^T \\right)_{\\mathrm{col~}j}\n",
    "\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるので，この結果を全要素に適用してあげて，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial X}\n",
    "&= \\left\\{ \\frac{\\partial l}{\\partial x_{ij}} \\right\\} \\\\\n",
    "&= \\left\\{\n",
    "\\mathrm{inner~product}\\left[\n",
    "\\left( \\frac{\\partial l}{\\partial A} \\right)_{\\mathrm{row}~i}, ~\n",
    "\\left( W^T \\right)_{\\mathrm{col~}j}\n",
    "\\right]\n",
    "\\right\\} \\\\\n",
    "&= \\frac{\\partial l}{\\partial A}~W^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，図 5-27 の四角 1 を導出できた．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine レイヤクラスの実装\n",
    "\n",
    "class Affine:\n",
    "    \"\"\" Affine layer\n",
    "    \n",
    "    A = XW + B  (a = xW + b のバッチ処理版) というアフィン変換に対応．\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        \"\"\" initialize\n",
    "        \n",
    "        Affine インスタンス作成時に パラメータ W,b の初期値を外部から指定できるので，\n",
    "        転移学習のときに便利かもしれない．他タスクで学習して得たパラ値を初期値にするやつ．\n",
    "        \n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\" backward propagation\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        return np.dot(X, self.W) + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" backward propagation\n",
    "        \"\"\"\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]]\n",
      "[15 18 21 24 27]\n",
      "[10 35 60]\n"
     ]
    }
   ],
   "source": [
    "# ↑の Affine レイヤ実装の試行錯誤\n",
    "\n",
    "tmp = np.arange(0,15).reshape(3,5)\n",
    "print( tmp )\n",
    "print( np.sum(tmp, axis=0) )\n",
    "print( np.sum(tmp, axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.3. Softmax-with-Loss レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p152] 図 5-28 に示すように、Softmax レイヤは、入力された値を正規化――出力の和が 1 になるように変形――して出力します。\n",
    "\n",
    "この表現みたいに「正規化」って言葉を「なんらかの性質を満たすように揃える」ていうフワっとした意味で使っている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p153] なお、ニューラルネットワークの正規化しない出力結果(図5-28 では Softmax の前層の Affine レ イヤの出力)は、「スコア」と呼ぶことがあります。つまり、ニューラルネット ワークの推論で答えをひとつだけ出す場合は、スコアの最大値だけに興味があ るため、Softmax レイヤは必要ない、ということです。一方、ニューラルネッ トワークの学習時には、Softmax レイヤが必要になります。\n",
    "\n",
    "これは前に出てきた話だが「スコア」という呼び方はわかりやすい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > [p153] これから Softmax レイヤを実装していきますが、ここでは、損失関数である交差 エントロピー誤差(cross entropy error)も含めて、「Softmax-with-Loss レイヤ」 という名前のレイヤで実装します。\n",
    " \n",
    " NN 全体の変換の流れは「Affine - Sigmoid - Affine - Sigmoid - ... - Affine - Sigmoid - Affine - Softmax - Loss」という感じ．なので，実装のイメージとして，\n",
    "\n",
    "- Affine レイヤ\n",
    "- Sigmoid レイヤ（一般には，Activate レイヤ）\n",
    "- Softmax - Loss レイヤ（一般には，最終 Activate - Loss レイヤ）\n",
    "\n",
    "という３種類の設計図(クラス)を作っておいて，そこからインスタンスを大量生産して，つなぎ合わせる，というのが筋が良い．図5-28みたいな感じで．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p153, 154] oftmax-with-Loss レイヤの導出過程に興味のある方は、「付録 A Softmax-with-Loss レイヤの計算グラフ」を参照してください。\n",
    "\n",
    "このへんのグラフの説明より数式のがわかりやすいので，自分で導出していく．\n",
    "\n",
    "ノーテーションは↓の通り．\n",
    "\n",
    "- $A$：最終アフィンレイヤの出力を表す行列．$(i,j)$ 成分が $i$ 番目のケースの $j$ 番目の特徴量(スコア)を表す．\n",
    "- $Y$：ソフトマックス関数で $A$ を処理して得られる行列．$(i,j)$ 成分が $i$ 番目のケースの $j$ 番目の確率値を表す．\n",
    "- $T$：処理対象のバッチのクラスラベルを one-hot-encoding で表した行列．$i$ 番目のケースがクラス $j$ に属する時 $t_{ij} = 1$ となり，それ以外のところでは $0$．\n",
    "\n",
    "さきほど導出した図 5-27 やパワポから分かるように，chain rule でロス関数のパラメータについての勾配を求める上で残り必要なのは $\\frac{\\partial l}{\\partial A}$ である．これを求めいこう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず，定義より\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial A} = \\left\\{ \\frac{\\partial l}{\\partial a_{ij}} \\right\\}\n",
    "$$\n",
    "\n",
    "であり，この $(i,j)$ 成分を求めていくと，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial a_{ij}}\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial y_{i1}} \\frac{\\partial y_{i1}}{\\partial a_{ij}} + \n",
    "\\frac{\\partial l}{\\partial y_{i2}} \\frac{\\partial y_{i2}}{\\partial a_{ij}} + \\cdots\n",
    "\\frac{\\partial l}{\\partial y_{iK}} \\frac{\\partial y_{iK}}{\\partial a_{ij}} \\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial y_{i1}} \\frac{\\partial}{\\partial a_{ij}}\n",
    "\\left( \\frac{\\exp(a_{i1})}{\\sum_{k=1}^K \\exp(a_{ik})} \\right) +\n",
    "\\frac{\\partial l}{\\partial y_{i2}}\n",
    "\\left( \\frac{\\exp(a_{i2})}{\\sum_{k=1}^K \\exp(a_{ik})} \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{iK}}\n",
    "\\left( \\frac{\\exp(a_{iK})}{\\sum_{k=1}^K \\exp(a_{ik})} \\right) \\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial y_{i1}} \\frac{1}{S^2}\n",
    "\\left( S \\frac{\\partial \\exp(a_{i1}) }{\\partial a_{ij}} - \\exp(a_{i1}) \\frac{\\partial S}{\\partial a_{ij}} \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{iK}} \\frac{1}{S^2}\n",
    "\\left( S \\frac{\\partial \\exp(a_{iK}) }{\\partial a_{ij}} - \\exp(a_{iK}) \\frac{\\partial S}{\\partial a_{ij}} \\right) \\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial y_{i1}} \\frac{1}{S^2}\n",
    "\\left(- \\exp(a_{i1}) \\frac{\\partial S}{\\partial a_{ij}} \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{ij}} \\frac{1}{S^2}\n",
    "\\left( S \\exp(a_{ij})  - \\exp(a_{ij}) \\frac{\\partial S}{\\partial a_{ij}} \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{iK}} \\frac{1}{S^2}\n",
    "\\left(- \\exp(a_{iK}) \\frac{\\partial S}{\\partial a_{ij}} \\right) \\\\\n",
    "&=\n",
    "\\frac{\\partial l}{\\partial y_{i1}} \\frac{1}{S^2}\n",
    "\\left(- \\exp(a_{i1}) \\exp(a_{ij}) \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{ij}} \\frac{1}{S^2}\n",
    "\\left( S \\exp(a_{ij})  - \\exp(a_{ij})^2 \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{iK}} \\frac{1}{S^2}\n",
    "\\left(- \\exp(a_{iK}) \\exp(a_{ij}) \\right) \\\\\n",
    "&=\n",
    "- \\frac{\\exp(a_{ij})}{S^2}\\left[\n",
    "\\frac{\\partial l}{\\partial y_{i1}}\n",
    "\\left(\\exp(a_{i1}) \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{ij}}\n",
    "\\left(\\exp(a_{ij})-S \\right) +\n",
    "\\cdots +\n",
    "\\frac{\\partial l}{\\partial y_{iK}}\n",
    "\\left(\\exp(a_{iK}) \\right)\n",
    "\\right] \\\\\n",
    "&=\n",
    "\\frac{\\exp(a_{ij})}{NS^2}\\left[\n",
    "\\frac{t_{i1}}{y_{i1}}\n",
    "\\left(\\exp(a_{i1}) \\right) +\n",
    "\\cdots +\n",
    "\\frac{t_{ij}}{y_{ij}}\n",
    "\\left(\\exp(a_{ij})-S \\right) +\n",
    "\\cdots +\n",
    "\\frac{t_{iK}}{y_{iK}}\n",
    "\\left(\\exp(a_{iK}) \\right)\n",
    "\\right] \\\\\n",
    "&=\n",
    "\\frac{\\exp(a_{ij})}{NS^2}\\left[\n",
    "\\frac{t_{i1} S}{\\exp(a_{i1})}\n",
    "\\left(\\exp(a_{i1}) \\right) +\n",
    "\\cdots +\n",
    "\\frac{t_{ij} S}{\\exp(a_{ij})}\n",
    "\\left(\\exp(a_{ij})-S \\right) +\n",
    "\\cdots +\n",
    "\\frac{t_{iK} S}{\\exp(a_{iK})}\n",
    "\\left(\\exp(a_{iK}) \\right)\n",
    "\\right] \\\\\n",
    "&=\n",
    "\\frac{\\exp(a_{ij})}{NS^2}\\left[\n",
    "\\sum_{k=1}^K t_{ik} S +\n",
    "\\frac{t_{ij} S}{\\exp(a_{ij})}\n",
    "\\left( - S \\right)\n",
    "\\right] \\\\\n",
    "&=\n",
    "\\frac{\\exp(a_{ij})}{NS} - \\frac{t_{ij}}{N} \\\\\n",
    "&=\n",
    "\\frac{1}{N}(y_{ij} - t_{ij})\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となる．式変形の等号を上から順に説明すると...\n",
    "1. $a_{ij}$ がケース $i$ の確率値 $y_{i1}, y_{i2}, \\ldots, y_{iK}$ を通してのみ $l$ に影響することを考えて，合成関数の微分 (chain rule) より．\n",
    "2. シグモイド関数の定義より．\n",
    "3. $S = \\sum_{k=1}^K \\exp(a_{ik})$ とおいて，分数の微分公式より．\n",
    "4. $\\frac{\\partial \\exp(a_{iu}) }{\\partial a_{ij}}$ は $u \\neq j$ のとき $0$ で $u = j$ のとき $\\exp(a_{ij})$．\n",
    "5. $\\frac{\\partial S}{\\partial a_{ij}} = \\exp(a_{ij})$\n",
    "6. くくり出した．\n",
    "7. クロスエントロピー誤差の定義 (4.3) より $\\frac{\\partial l}{\\partial y_{iu}} = \\frac{\\partial}{\\partial y_{iu}}\\left(-\\frac{1}{N}\\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\log y_{nk}\\right) = -\\frac{1}{N} \\frac{t_{iu}}{y_{iu}}$ となるから．\n",
    "8. シグモイド関数の定義より．\n",
    "9. まとめただけ．\n",
    "10. one-hot-encoding なので $\\sum_{k=1}^K t_{ik} = 1$より．\n",
    "11. シグモイド関数の定義より．\n",
    "\n",
    "である．この結果を全ての成分に適用してあげて，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial A}\n",
    "&=\n",
    "\\left\\{ \\frac{\\partial l}{\\partial a_{ij}} \\right\\} \\\\\n",
    "&=\n",
    "\\left\\{ \\frac{1}{N} (y_{ij} - t_{ij}) \\right\\} \\\\\n",
    "&=\n",
    "\\frac{1}{N} (Y - T)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，図 5-30 の勾配を導くことができた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p154, 155] 図 5-30 で注目すべきは、逆伝播の結果です。Softmax レイヤからの逆伝播は、 $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$ という“キレイ”な結果になっています。... ニューラルネットワーク の逆伝播では、この差分である誤差が前レイヤへ伝わっていくのです。これはニュー ラルネットワークの学習における重要な性質です。... “キレイ”な結果は偶然ではなく、そうなるように交差エント ロピー誤差という関数が設計されたのです。また、回帰問題では出力層に「恒 等関数」を用い、損失関数として「2 乗和誤差」を用いますが(「3.5 出力層の 設計」参照)、これも同様の理由によります。つまり、「恒等関数」の損失関数 として「2 乗和誤差」を用いると、逆伝播が $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$ とい う“キレイ”な結果になるのです。\n",
    "\n",
    "ネットワーク(変換プロセス)の後ろの方から，誤差 $Y-T$ の算出を起点として，途中結果を受け継ぎながら chain rule で勾配を効率よく計算していく．だから誤差逆伝搬法 (back propagation) という名前なのか．\n",
    "\n",
    "\"キレイ\"な結果になっているメリットは計算速度．勾配を使った最適化(SGDとか)では更新回数の分だけ back propagation をしないといけない訳だから $Y-T$ という簡単な引き算は全体で見ると大きなメリット．\n",
    "\n",
    "あと「最終 activate 関数とロス関数の相性が大事」というのは，ベイズ理論の共役事前分布(尤度関数と相性が良い事前分布)の話と似ているな．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Softmax - with - Loss レイヤの実装\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" initialize\n",
    "        \"\"\"\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        self.loss = None  # loss 値を状態で持つ意味は無いかも．勾配計算で使わないし．\n",
    "        \n",
    "    def forward(self, a, t):\n",
    "        \"\"\" forward propagation\n",
    "        \"\"\"\n",
    "        self.y = softmax(a)\n",
    "        self.t = t\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\" backward propagation\n",
    "        \"\"\"\n",
    "        da = (self.y - self.t) / self.t.shape[0]\n",
    "        return da\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" softmax function\n",
    "    \n",
    "    from score to probability\n",
    "    \n",
    "    \"\"\"\n",
    "    if x.ndim==1:\n",
    "        x = x.reshape(1, x.shape[0])\n",
    "    c = np.max(x, axis=1, keepdims=True)     # オーバーフロー対策\n",
    "    return np.exp(x - c)/np.sum(np.exp(x - c), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\" calculate cross entropy error\n",
    "    \n",
    "    Args:\n",
    "      y: ソフトマックス関数が出力した確率値\n",
    "      t: 正解クラスの one-hot-encoding\n",
    "    \n",
    "    \"\"\"\n",
    "    if y.ndim == 1:  # ndim=1 つまり単一観測値が入力されたら ndim=2(行列) に整形\n",
    "        y = y.reshape(1, y.shape[0])\n",
    "        t = t.reshape(1, t.shape[0])\n",
    "    return - (1/y.shape[0]) * np.sum(t * np.log(y + 1e-7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03963479866051434 0.03963479866051434\n",
      "[[0.01714783 0.93623955 0.04661262]\n",
      " [0.98670329 0.00664835 0.00664835]]\n",
      "[[0 1 0]\n",
      " [1 0 0]]\n",
      "[[ 0.00857391 -0.03188022  0.02330631]\n",
      " [-0.00664835  0.00332418  0.00332418]]\n",
      "0.06588379694716416 0.06588379694716416\n",
      "[[0.01714783 0.93623955 0.04661262]]\n",
      "[0 1 0]\n",
      "[[ 0.00571594 -0.02125348  0.01553754]]\n"
     ]
    }
   ],
   "source": [
    "# ↑の Softmax - with - Loss レイヤの動作確認\n",
    "\n",
    "\n",
    "sfls = SoftmaxWithLoss()\n",
    "\n",
    "loss = sfls.forward(\n",
    "    a=np.array([[1.0, 5.0, 2.0], [6.0, 1.0, 1.0]]),\n",
    "    t=np.array([[0, 1, 0], [1, 0, 0]])\n",
    ")\n",
    "print(loss, sfls.loss)\n",
    "print(sfls.y)\n",
    "print(sfls.t)\n",
    "grad = sfls.backward()\n",
    "print(grad)\n",
    "\n",
    "loss = sfls.forward(\n",
    "    a=np.array([1.0, 5.0, 2.0]),\n",
    "    t=np.array([0, 1, 0])\n",
    ")\n",
    "print(loss, sfls.loss)\n",
    "print(sfls.y)\n",
    "print(sfls.t)\n",
    "grad = sfls.backward()\n",
    "print(grad)\n",
    "\n",
    "# ok．勾配の政負も直感に合ってる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]] \n",
      " [[ 0  4  8]\n",
      " [ 1  5  9]\n",
      " [ 2  6 10]\n",
      " [ 3  7 11]]\n",
      "11\n",
      "[ 3  7 11]\n",
      "(3, 4) (3,)\n",
      "[[ 3]\n",
      " [ 7]\n",
      " [11]]\n",
      "[[-3 -2 -1  0]\n",
      " [-3 -2 -1  0]\n",
      " [-3 -2 -1  0]]\n",
      "(3, 4) (3, 1)\n",
      "[[1.00000000e+00 2.71828183e+00 7.38905610e+00 2.00855369e+01]\n",
      " [5.45981500e+01 1.48413159e+02 4.03428793e+02 1.09663316e+03]\n",
      " [2.98095799e+03 8.10308393e+03 2.20264658e+04 5.98741417e+04]]\n",
      "66\n",
      "[ 6 22 38]\n",
      "(3, 4) (3,)\n",
      "[[ 6]\n",
      " [22]\n",
      " [38]]\n",
      "[[0.         0.16666667 0.33333333 0.5       ]\n",
      " [0.18181818 0.22727273 0.27272727 0.31818182]\n",
      " [0.21052632 0.23684211 0.26315789 0.28947368]]\n",
      "(3, 4) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# ↑の Softmax - with - Loss レイヤ実装の試行錯誤\n",
    "\n",
    "\n",
    "mat = np.arange(0, 12).reshape(3,4)\n",
    "print(mat, \"\\n\", mat.T)  # 転置\n",
    "\n",
    "\n",
    "print( np.max(mat) )  # 全体の max になってしまう．\n",
    "print( np.max(mat, axis=1))  # ケース(行)ごとの max をとる\n",
    "c = np.max(mat, axis=1)  \n",
    "# print( mat - c )  # エラー．ブロードキャストできない．\n",
    "print( mat.shape, c.shape )  # ndim が異なってるからか．\n",
    "c = np.max(mat, axis=1, keepdims=True)  # ndim を変えさせない．\n",
    "print( c )\n",
    "print( mat - c )   # ブロードキャストできた．\n",
    "print( mat.shape, c.shape )  # ndim が揃ってると大丈夫なのか．\n",
    "\n",
    "\n",
    "print( np.exp(mat) )  # 全要素に exp してくれるので OK．\n",
    "\n",
    "\n",
    "print( np.sum(mat) )  # 全体の和になってしまう．\n",
    "print( np.sum(mat, axis=1) )  # ケース(行)ごとの和になった\n",
    "# print( mat / np.sum(mat, axis=1))  # さっきの引き算と同様これだとブロードキャスト効かない．\n",
    "print( mat.shape, np.sum(mat, axis=1).shape )  # ndim が異なるからか．\n",
    "print( np.sum(mat, axis=1, keepdims=True) )     # ndim を変化させないと\n",
    "print( mat / np.sum(mat, axis=1, keepdims=True) )  # 無事にブロードキャストできる．\n",
    "print( mat.shape, np.sum(mat, axis=1, keepdims=True).shape)  # ndim が揃ってるから大丈夫なんだな．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7. 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p156] 前節で実装したレイヤを組み合わせることで、まるでレゴブロックを組み合わせて 作るように、ニューラルネットワークを構築することができます。\n",
    "\n",
    "レイヤの設計図(テンプレ,雛形)をクラス `Affine`, `Sigmoid`, `SoftmaxWithLoss`で実装したので，あとはそこから必要な分だけインスタンスを作成して，つなぎ合わせれば良い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p157] 誤差逆伝播法が登場するのは、ステップ 2 の「勾配の算出」です。... 数値微分は簡単に実装できる反面、計算に多くの時間がかかりました。誤差逆伝播法を用いれば、時間を要する数値微分とは違い、高速に効率良く勾配を求めることができます。\n",
    "\n",
    "繰り返しだが，ここは重要．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.2. 誤差逆伝播法に対応したニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p157] レイヤを使用することによって、認識結果を得る処理\n",
    "(predict())や勾配を求める処理(gradient())がレイヤの伝播だけで達成できます。\n",
    "\n",
    "レイヤ単位の実装のメリット．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5.1. の ipynb で作成した数値微分を求める関数\n",
    "# 次の TwoLayerNet クラスで使うので定義しておく．\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    \"\"\"数値的に勾配ベクトルを求める\n",
    "    \n",
    "    これまでに作った numerical_gradient() はベクトルを引数に取る関数だけが対象だった．\n",
    "    ここでは，行列(重み係数W)を引数に取る関数(損失関数)の勾配も求められるようにする．\n",
    "    Args:\n",
    "      f: 勾配を求めたい関数．\n",
    "      x: 勾配を求めたい地点 \n",
    "    \n",
    "    \"\"\"\n",
    "    h = 1e-4                            # 丸め誤差に注意し小さすぎない値\n",
    "    grad = np.zeros_like(x)  #  勾配は変数と同じ形状．入力が行列な関数には勾配行列を求める．\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'])  # 勾配を求めたい座標値 x をイテレータ化．\n",
    "    \n",
    "    while not it.finished:           # イテレータなら ndim によらず1 重ループで書ける．\n",
    "        \n",
    "        idx = it.multi_index        # 元の行列 x の何行何列か\n",
    "        tmp_val = x[idx]            # 変数 x_{idx} の元の値を逃しておく\n",
    "        \n",
    "        x[idx] = tmp_val + h     # 変数 x_{idx} を元の値からちょっとプラスにずらして，\n",
    "        fxh1 = f(x)                     # その位置での関数値を取得．\n",
    "        \n",
    "        x[idx] = tmp_val - h      # 変数 x_{idx} を元の値からちょっとマイナスにずらして ，\n",
    "        fxh2 = f(x)                     # その位置での関数値を取得．\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)    # 中心差分で偏微分係数を近似算出\n",
    "        \n",
    "        x[idx] = tmp_val            # 変数 x_{idx} の値を元に戻す．\n",
    "        \n",
    "        it.iternext()                    # イテレータを１つ進めて次の変数を見るようにする． \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ２層ニューラルネットワークの実装 (back propagation アリ)\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\" 2層ニューラルネットワークのクラス\n",
    "    \n",
    "    Affine - Sigmoid - ReLU - Softmax (- CrossEntropyLoss)\n",
    "    \n",
    "    入力・出力・中間層のニューロン数は一般化している．\n",
    "    最適化はメソッドにはせず外で回すので，その手法(optimizer)は未指定．\n",
    "    層の数，活性化関数，ロス関数などは固定． 分類タスク用の浅い NN ．\n",
    "    \n",
    "    Attributes:\n",
    "      layers: NN を構成するレイヤのインスタンスを持った順序付きディクショナリ．\n",
    "      lastLayer: NN を構成する最後尾の「最終活性化関数-ロス関数」レイヤのインスタンス．\n",
    "      params: パラメータ W1, b1, W2, b2 の値を持ったディクショナリ． 各 layer インスタンス内の状態と共有してる\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \"\"\" initialize\n",
    "        \n",
    "        Args:\n",
    "          input_size: 入力層のノード数（生の説明変数の次元）\n",
    "          hidden_size: 中間層のノード数(学習で獲得させる特徴量の次元)\n",
    "          output_size: 出力層のノード数（アウトカムの次元）\n",
    "          weight_init_std: 重み係数wの初期値を発生させる正規分布の標準偏差\n",
    "\n",
    "        Note: \n",
    "          パラ値は layers 内のインスタンスの状態として持っておけば通常十分だが，\n",
    "          gradient check のためのナイーブな数値微分のときのために params にも持っておく．\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = np.random.normal(\n",
    "            loc=0, scale=weight_init_std, size=(input_size, hidden_size)\n",
    "        )\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = np.random.normal(\n",
    "            loc=0, scale=weight_init_std, size=(hidden_size, output_size)\n",
    "        )\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W1\"], self.params[\"b1\"])\n",
    "        self.layers[\"Relu1\"] = ReLU()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" prediction by forward propagation\n",
    "        \n",
    "        Args:\n",
    "          x: 説明変数ベクトル\n",
    "        \n",
    "        Returns:\n",
    "          np.array: 行がケースで列がクラスに対応するスコア行列．\n",
    "        \n",
    "        Note: \n",
    "          計算コストを考慮し，スコアを返すようにする． softmax() で確率に変換まではしない．\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\" calculate loss\n",
    "        \n",
    "        現状のパラ値でのロス値を取得．つまり，フルで forward を実行する．\n",
    "        back propagation 直前にも使うし，学習経過の記録にも使える．\n",
    "\n",
    "        Args:\n",
    "          x: 損失を計算したい対象データ(ミニバッチ)の説明変数\n",
    "          t: そのクラスラベル\n",
    "          \n",
    "        Returns:\n",
    "          float: ロス値\n",
    "\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\" calculate accuracy\n",
    "        \n",
    "        現状のパラ値での Accuracy を取得．\n",
    "        作ったモデルの汎化性能の評価にも使えるし，学習経過の記録にも使える．\n",
    "\n",
    "        Args:\n",
    "          x: Accuracy を計算したい対象のデータの説明変数\n",
    "          t: そのクラスラベル\n",
    "          \n",
    "        Returns: \n",
    "          float: Accuracy.\n",
    "\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)    # 確率最大クラスを割り当て\n",
    "        if t.ndim != 1:                        # one-hot-encoding な t　をクラスラベルに直す \n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\" compute gradients by backward propagation\n",
    "        \n",
    "        パラメータ W1,W2,b1,b2 について，現状の値でのロス関数の勾配を算出する．        \n",
    "        backward propagation で効率的に実行する．\n",
    "        それを返却して，外での更新アルゴリズム (SGD とか) で利用する．\n",
    "        \n",
    "        Args: \n",
    "          x: このミニバッチデータでの損失関数の勾配を求める．計算効率のための近似．\n",
    "          t: そのクラスラベル\n",
    "          \n",
    "        Returns:\n",
    "          dict: 各パラメータの勾配の numpy.array． 学習時のために params と同じ keys にしとく．\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss(x, t)                         # 1回だけフルで forward して途中経過を状態に保存．\n",
    "        dout = self.lastLayer.backward()\n",
    "        layers = list(self.layers.values())  # python の代入はオブジェクト共有(アドレス渡し)\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine2\"].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\" compute gradients by naive numerical differential\n",
    "        \n",
    "        数値微分でナイーブに勾配を求める．\n",
    "        back propagation による勾配算出の検証 (gradient check) に使うだけ．\n",
    "        \n",
    "        Args: \n",
    "          x: このミニバッチデータでの損失関数の勾配を求める．計算効率のための近似．\n",
    "          t: そのクラスラベル\n",
    "          \n",
    "        Returns:\n",
    "          dict: 各パラメータの勾配の numpy.array． 学習時のために params と同じ keys にしとく．\n",
    "          \n",
    "        Note:\n",
    "          挙動の詳細については chapter 4 の ipynb を参照．\n",
    "        \n",
    "        \"\"\"\n",
    "        loss_fn = lambda _: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(f=loss_fn, x=self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(f=loss_fn, x=self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(f=loss_fn, x=self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(f=loss_fn, x=self.params[\"b2\"])\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['Affine1', 'Relu1', 'Affine2']) odict_values([<__main__.Affine object at 0x128b35b10>, <__main__.ReLU object at 0x128b35e10>, <__main__.Affine object at 0x128b350d0>])\n",
      "[[ 0.00752177  0.00097072 -0.01955908 -0.00987035]\n",
      " [ 0.00871967 -0.00034007 -0.00750571  0.02010199]\n",
      " [ 0.00853875 -0.00601798  0.01596879 -0.00490602]\n",
      " [ 0.0113944   0.00182417 -0.01493994 -0.01046746]\n",
      " [-0.0042657  -0.00222068  0.005351   -0.00302954]]\n",
      "[0. 0. 0. 0.]\n",
      "[[ 0.00657984 -0.00018661 -0.00461439]\n",
      " [-0.01395028 -0.0063379   0.01336234]\n",
      " [-0.01087865 -0.00428024 -0.01921184]\n",
      " [ 0.00996627  0.00881128 -0.00583273]]\n",
      "[0. 0. 0.]\n",
      "[[ 0.00657984 -0.00018661 -0.00461439]\n",
      " [-0.01395028 -0.0063379   0.01336234]\n",
      " [-0.01087865 -0.00428024 -0.01921184]\n",
      " [ 0.00996627  0.00881128 -0.00583273]]\n",
      "[[-0.94056344  1.0928207  -0.37959991  0.9553718  -0.19716478]\n",
      " [ 0.87460205 -0.78569515 -0.25244162  1.40332204  0.92267707]]\n",
      "[[ 2.63925292e-04  1.86724984e-04 -1.46283218e-04]\n",
      " [ 1.94468922e-05 -2.17372399e-05 -2.37703116e-06]]\n",
      "[[1 0 0]\n",
      " [0 0 1]]\n",
      "1.0985311724990936\n",
      "0.5\n",
      "{'W1': array([[ 0.00509238, -0.01232685,  0.        ,  0.00265712],\n",
      "       [-0.00531661,  0.01251633,  0.        , -0.00308725],\n",
      "       [ 0.00047889, -0.00023119,  0.        ,  0.00107238],\n",
      "       [ 0.00079432, -0.00543565,  0.        , -0.00269895],\n",
      "       [ 0.0029925 , -0.0083771 ,  0.        ,  0.000557  ]]), 'b1': array([-0.00038939, -0.00201569,  0.        , -0.00282503]), 'W2': array([[-0.00204196,  0.00342781, -0.00138585],\n",
      "       [-0.00053564,  0.00105448, -0.00051884],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-0.00790299,  0.00395215,  0.00395083]]), 'b2': array([-0.16663609,  0.33334418, -0.16670809])}\n",
      "{'W1': array([[ 0.00509238, -0.01232685,  0.        ,  0.00265712],\n",
      "       [-0.00531661,  0.01251633,  0.        , -0.00308725],\n",
      "       [ 0.00047889, -0.00023119,  0.        ,  0.00107238],\n",
      "       [ 0.00079432, -0.00543565,  0.        , -0.00269895],\n",
      "       [ 0.0029925 , -0.0083771 ,  0.        ,  0.000557  ]]), 'b1': array([-0.00038939, -0.00201569,  0.        , -0.00282503]), 'W2': array([[-0.00204196,  0.00342781, -0.00138585],\n",
      "       [-0.00053564,  0.00105448, -0.00051884],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-0.00790298,  0.00395215,  0.00395083]]), 'b2': array([-0.16663604,  0.33334408, -0.16670804])}\n"
     ]
    }
   ],
   "source": [
    "# ↑の TwoLayerNet の動作確認．\n",
    "\n",
    "network = TwoLayerNet(input_size=5, hidden_size=4, output_size=3)\n",
    "print( network.layers.keys(), network.layers.values() )\n",
    "print( network.layers[\"Affine1\"].W)  # 重みパラの初期値 (5,4)\n",
    "print( network.layers[\"Affine1\"].b)   # バイアスパラの初期値 (,5)\n",
    "print( network.layers[\"Affine2\"].W)  # 重みパラの初期値 (4,3)\n",
    "print( network.layers[\"Affine2\"].b)   # バイアスパラの初期値 (,3)\n",
    "print( network.params[\"W2\"] )          # 直下の状態 params と共有してる．\n",
    "\n",
    "x = np.random.normal(size=(2, 5))\n",
    "print(x)\n",
    "score = network.predict(x)\n",
    "print(score)\n",
    "\n",
    "t = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "print(t)\n",
    "loss = network.loss(x, t)\n",
    "print(loss)\n",
    "\n",
    "acc = network.accuracy(x, t)\n",
    "print(acc)  # さっきの print(score) と print(t) の出力と比較して，あってる．\n",
    "\n",
    "grad_bp = network.gradient(x, t)\n",
    "print(grad_bp)\n",
    "grad_naive = network.numerical_gradient(x, t)\n",
    "print(grad_naive)\n",
    "# ちゃんと back propagation で算出した勾配とナイーブに算出した勾配が一致．この確認が gradient check．\n",
    "# よくよく考えるとすごいこと． chain rule の良い応用例．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 100) (3, 5)\n",
      "(3, 50)\n",
      "(3, 50)\n",
      "スコア [[-6.21945603e-05 -3.87004899e-03  3.50329478e-03  1.04831512e-02\n",
      "   3.27972499e-03]\n",
      " [-4.75261879e-03 -8.87727205e-03  2.57618808e-03  3.99705173e-03\n",
      "   9.80530097e-04]\n",
      " [-5.87962380e-03 -2.14921697e-03 -1.49415816e-03  1.03302451e-02\n",
      "  -1.99788051e-03]]\n",
      "(3, 5)\n",
      "確率値 [[0.19945271 0.19869466 0.20016512 0.20156713 0.20012038]\n",
      " [0.19929143 0.19847111 0.20075736 0.20104281 0.20043728]\n",
      " [0.19887185 0.19961511 0.19974591 0.20212181 0.19964532]]\n",
      "(3, 5) [1. 1. 1.]\n",
      "1.612777672074524\n"
     ]
    }
   ],
   "source": [
    "# ↑のTwoLayerNet 実装の試行錯誤\n",
    "# 主に forward 方向について．\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "hidden_size = 50\n",
    "output_size = 5\n",
    "\n",
    "# パラメータ初期値の設定．\n",
    "W1 = np.random.normal(loc=0, scale=0.01, size=(input_size, hidden_size))\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = np.random.normal(loc=0, scale=0.01, size=(hidden_size, output_size))\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# レイヤインスタンスを作成．\n",
    "affine1 = Affine(W1, b1)\n",
    "relu1 = ReLU()\n",
    "affine2 = Affine(W2, b2)\n",
    "last = SoftmaxWithLoss()\n",
    "\n",
    "\n",
    "# サンプルデータを適当に用意．\n",
    "x = np.random.normal(size=(3, 100))\n",
    "t = np.array([[0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0]])\n",
    "# print( x )\n",
    "# print( t )\n",
    "print( x.shape, t.shape )\n",
    "\n",
    "a1 = affine1.forward(x)\n",
    "# print( a1 )\n",
    "print( a1.shape )\n",
    "z1 = relu1.forward(a1)\n",
    "# print( z1 )\n",
    "print( z1.shape )\n",
    "# 1層目の Affine, ReLU (活性化関数) レイヤによる変換．OK．\n",
    "\n",
    "a2 = affine2.forward(z1)                  \n",
    "print( \"スコア\", a2 )  # スコア(softmax 変換前)． \n",
    "print( a2.shape )\n",
    "# 2層目(最終層)の Affine レイヤによる変換．\n",
    "# 予測の時はこの affine2.forward() まででOK　．分類したいだけだから．\n",
    "\n",
    "l = last.forward(a2, t)\n",
    "print( \"確率値\", last.y)  # スコアを softmax で変換した確率値\n",
    "print( last.y.shape, np.sum(last.y, axis=1) )\n",
    "print( l )                          # ロス値． \n",
    "# Softmax (最後の活性化関数) レイヤによる確率値の取得と，\n",
    "# その結果と正解クラスをもとにしたロス値の算出．\n",
    "# 学習経過記録とかナイーブ勾配算出でロス値を見たい時は，この最終 .forward()も必要．\n",
    "# back propagation のはじめに  d(ロス) / d(スコア) を算出する必要があるが，\n",
    "# そのために１回フルで forward してスコア値と正解クラスを記録させておく必要があるので，\n",
    "# そのときもこの最終 .forward() までやる必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "確率値 y： [[0.19945271 0.19869466 0.20016512 0.20156713 0.20012038]\n",
      " [0.19929143 0.19847111 0.20075736 0.20104281 0.20043728]\n",
      " [0.19887185 0.19961511 0.19974591 0.20212181 0.19964532]]\n",
      "正解クラス t： [[0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]]\n",
      "dl/da2 = (y - t)/n： [[ 0.06648424 -0.26710178  0.06672171  0.06718904  0.06670679]\n",
      " [ 0.06643048  0.06615704  0.06691912  0.06701427 -0.26652091]\n",
      " [-0.26704272  0.06653837  0.06658197  0.06737394  0.06654844]]\n",
      "dl/dz1 = dl/da2 w2^t： (3, 50)\n",
      "dl/dW2 = z1^t dl/da2： (50, 5)\n",
      "dl/db2 = sum_by_col(dl/da2)： (5,)\n",
      "dl/da1 = dl/dz1* dz1/da1 = dl/dz1*[a1>0?1:0]： (3, 50)\n",
      "[[-3.96511960e-03 -5.84182725e-04  6.70814489e-04]\n",
      " [-3.46996114e-04  9.03745550e-05  6.61736849e-03]\n",
      " [ 4.88808319e-03 -2.55162685e-04  3.29357026e-03]] \n",
      " [[-0.12196562  0.03476752 -0.02588252]\n",
      " [-0.02848035 -0.0400615  -0.17471189]\n",
      " [ 0.17449035  0.02776314  0.09534877]] \n",
      " [[-0.         -0.00058418  0.        ]\n",
      " [-0.          0.          0.        ]\n",
      " [ 0.00488808 -0.00025516  0.00329357]]\n",
      "dl/dx = dl/da1 w1^t： (3, 100)\n",
      "dl/dw1 = X^t dl/da1： (100, 50)\n",
      "dl/db1 = sum_by_col(dl/da1)： (50,)\n"
     ]
    }
   ],
   "source": [
    "# ↑のTwoLayerNet 実装の試行錯誤\n",
    "# forward をフルで実行したあとの backward 勾配算出．\n",
    "\n",
    "\n",
    "da2 = last.backward()\n",
    "print(\"確率値 y：\", last.y)\n",
    "print(\"正解クラス t：\", last.t)\n",
    "print(\"dl/da2 = (y - t)/n：\", da2)\n",
    "# SoftmaxWithLoss レイヤでの backward 処理．\n",
    "# このレイヤには推定パラメータは無いが， 勾配(誤差) dl/da2 は逆伝播させて chain rule に使いたい．\n",
    "# なのでこの dl/da2 は状態に保存ではなくちゃんと返却値として得られるようにしてある．\n",
    "\n",
    "dz1 = affine2.backward(dout=da2)\n",
    "print(\"dl/dz1 = dl/da2 w2^t：\", dz1.shape)\n",
    "# 逆伝播させて chain rule に使う勾配 dl/dz1． shape は a2 が (3, 5), w2が (50, 5) で OK．\n",
    "# よってこれは状態に保存しておくんじゃなくちゃんと返却値として得られるようになってる．\n",
    "print(\"dl/dW2 = z1^t dl/da2：\", affine2.dW.shape)\n",
    "# dl/dW2 は推定パラメータの勾配なので，状態に保存しておいてあとで勾配降下に使う．\n",
    "# shape は z1 が (3, 50) で a2 が (3, 5) なので dl/dW2 が (50, 5)となり，合ってる．\n",
    "print(\"dl/db2 = sum_by_col(dl/da2)：\", affine2.db.shape)\n",
    "# dl/db2 は推定パラメータの勾配なので，状態に保存しておいてあとで勾配降下に使う．\n",
    "# shape は a2 が (3,5) なので，列についての和をとって dl/db2　が5次元ベクトルとなり，合ってる．\n",
    "\n",
    "da1 = relu1.backward(dout=dz1)\n",
    "print(\"dl/da1 = dl/dz1* dz1/da1 = dl/dz1*[a1>0?1:0]：\", da1.shape)\n",
    "# 逆伝搬させて chain rule に使う勾配 dl/da1 は返却値として取得できるように．\n",
    "# shape は z1 が (3, 50), a1 が (3, 50) でその要素単位の積で dl/da1 も (3, 50) でOK．\n",
    "print(dz1[:3,:3], \"\\n\", relu1.x[:3,:3], \"\\n\", da1[:3, :3])\n",
    "# うん，確かに導出した式通りの chain rule 計算できてる．\n",
    "\n",
    "dx = affine1.backward(dout=da1)\n",
    "print(\"dl/dx = dl/da1 w1^t：\", dx.shape)\n",
    "# これ以上 back のレイヤは無いから，もうこの dx は逆伝搬させる必要もないが，一応．\n",
    "print( \"dl/dw1 = X^t dl/da1：\", affine1.dW.shape )\n",
    "print( \"dl/db1 = sum_by_col(dl/da1)：\", affine1.db.shape )\n",
    "# dl/dw1, dl/db1 は推定パラメータ勾配でこれを使ってあとで勾配降下するので，状態に保存．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "OrderedDict([('a', [1, 2, 3]), ('b', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))])\n",
      "[1, 2, 3]\n",
      "[[1, 2, 3], array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])]\n",
      "[1, 2, 3]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[[1, 2, 3], array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])]\n",
      "[1, 2, 3]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# ↑のTwoLayerNet 実装の試行錯誤\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "print( OrderedDict )\n",
    "od = OrderedDict()\n",
    "od[\"a\"] = [1,2,3]\n",
    "od[\"b\"] = np.arange(10)\n",
    "print( od )\n",
    "print( od[\"a\"] )\n",
    "print( list(od.values()) )\n",
    "for i in od.values():\n",
    "    print(i)\n",
    "\n",
    "dic = {}\n",
    "dic[\"a\"] = [1,2,3]\n",
    "dic[\"b\"] = np.arange(10)\n",
    "print( list(dic.values()) )\n",
    "for i in dic.values():\n",
    "    print(i)\n",
    "\n",
    "# この単純な操作だと問題ないが， Python 3.6 以前では，\n",
    "# dictionary では要素の順番が保存されない．なんかの拍子に入れ替わったりする．\n",
    "# そのため，順序が保存される OrderedDict が使われてきた．\n",
    "# 今( Python 3.7 以降) の dictionary ではこれが起きないけど，\n",
    "# 実行される環境依存になるのは良くないので，安全をとって　Ordered で実装すべき．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p160] Affine レイヤや ReLU レイヤが、それぞれの内部で順伝播と逆伝播を正しく処理してくれるので、こ こで行うことは、レイヤを正しい順番で連結し、順番に(もしくは逆順に)レイヤを呼び出すだけなのです。\n",
    "このようにニューラルネットワークの構成要素を「レイヤ」として実装したことで、ニューラルネットワークを簡単に構築することができました。この「レイヤ」と してモジュール化する実装の利点は絶大です。なぜなら、もし別のネットワーク―― たとえば、5 層、10 層、20 層、...と大きなネットワーク――を作りたいなら、単に必 要なレイヤを追加するだけでニューラルネットワークを作ることができるのです(ま るでレゴブロックを組み立てるように)。後は、各レイヤの内部で実装された順伝播 と逆伝播によって、認識処理や学習に必要な勾配が正しく求められます。\n",
    "\n",
    "これが，ここでの実装の指針．集中講義で使った Pytorch もこういう思想の仕様だった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.3. 誤差逆伝播法の勾配確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [p161] 数値微分の利点は、実装が簡単であるということです。そのため、数値微分の実装 はミスが起きにくく、一方、誤差逆伝播法の実装は複雑になるためミスが起きやすいのが一般的です。そこで、数値微分の結果と誤差逆伝播法の結果を比較して、誤差逆伝播法の実装の正しさを確認することがよく行われます。... **勾配確認** (gradient check) と言います．\n",
    "\n",
    "これは自分で実装してみてめっちゃ思った．勾配確認(gradient check) は自然にしたくなる．実際 TwoLayerNet の挙動確認のとき少しやった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 784) (3, 10)\n",
      "W1：4.888248587153901e-10\n",
      "b1：2.884188633838457e-09\n",
      "W2：5.239987748665936e-09\n",
      "b2：1.3942655267412675e-07\n"
     ]
    }
   ],
   "source": [
    "# MNIST を一部使って gradient check\n",
    "\n",
    "(x_train, t_train), (_, __) = load_mnist(normalize=True, one_hot_label=True)\n",
    "x_batch = x_train[:3, ]\n",
    "t_batch = t_train[:3, ]\n",
    "print(x_batch.shape, t_batch.shape)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "# 各パラメータ値の差を MAE 的な感じで測る．\n",
    "for key in grad_backprop.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \"：\" + str(diff))\n",
    "    \n",
    "# 十分小さい． backward propagation を正しく導出・実装できていることを確認できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.4. 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : train acc, test acc | 0.11725, 0.1219\n",
      "epoch 1 : train acc, test acc | 0.9037166666666666, 0.9081\n",
      "epoch 2 : train acc, test acc | 0.9222166666666667, 0.9271\n",
      "epoch 3 : train acc, test acc | 0.93555, 0.9377\n",
      "epoch 4 : train acc, test acc | 0.9428, 0.9429\n",
      "epoch 5 : train acc, test acc | 0.9508666666666666, 0.9495\n",
      "epoch 6 : train acc, test acc | 0.9560166666666666, 0.9538\n",
      "epoch 7 : train acc, test acc | 0.96085, 0.9575\n",
      "epoch 8 : train acc, test acc | 0.96275, 0.9595\n",
      "epoch 9 : train acc, test acc | 0.9668666666666667, 0.9619\n",
      "epoch 10 : train acc, test acc | 0.9690166666666666, 0.9637\n",
      "epoch 11 : train acc, test acc | 0.9708, 0.9642\n",
      "epoch 12 : train acc, test acc | 0.9730333333333333, 0.9654\n",
      "epoch 13 : train acc, test acc | 0.9737666666666667, 0.9676\n",
      "epoch 14 : train acc, test acc | 0.9753666666666667, 0.9699\n",
      "epoch 15 : train acc, test acc | 0.9764, 0.9703\n",
      "epoch 16 : train acc, test acc | 0.9778, 0.9686\n",
      "Final Test Accuracy : 0.972\n",
      "CPU times: user 1min 7s, sys: 5.11 s, total: 1min 12s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 誤差逆伝播を利用した効率的な SGD の実装．\n",
    "\n",
    "\n",
    "# データの取得\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "# print(x_train.shape, t_train.shape, x_test.shape, t_test.shape)\n",
    "\n",
    "\n",
    "# NN モデルオブジェクトの作成(初期化)\n",
    "network = TwoLayerNet(\n",
    "    input_size=x_train.shape[1], hidden_size=50, output_size=t_train.shape[1]\n",
    ")\n",
    "\n",
    "\n",
    "# 学習 (SGD) の設定 (ある種のハイパーパラメータ)\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "# サイズの取得\n",
    "train_size = x_train.shape[0]\n",
    "iter_per_epoch = round(max(train_size / batch_size, 1))  # = 600\n",
    "\n",
    "\n",
    "# 各種記録用の箱を作成\n",
    "train_loss_list = []             # 訓練データ(バッチ)に対するロス\n",
    "train_acc_list = []              # 訓練データに対する Accuracy\n",
    "test_acc_list = []               # テストデータに対する Accuracy\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "    \n",
    "    # ミニバッチのサンプリング\n",
    "    idx = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[idx, :]\n",
    "    t_batch = t_train[idx, :]\n",
    "    \n",
    "    # backward propagatin で勾配を算出．\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # gradient descent\n",
    "    for key in network.params.keys():\n",
    "        network.params[key] -= learning_rate * grads[key]\n",
    "        # .params は .layer とオブジェクトを共有しているので， 片方を更新すれば問題ない．\n",
    "    \n",
    "    # 訓練ロスを記録しとく．\n",
    "    train_loss_list.append(network.loss(x_batch, t_batch))\n",
    "    \n",
    "    # epoch ごとに訓練/テストデータに対する Accuracy を記録\n",
    "    if i % iter_per_epoch == 0:  # 「何エポック目か」が整数なら記録．添字が0始まりなのを考慮するなら　i+1 が厳密かも．\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"epoch \" + str(i // iter_per_epoch) + \" : \" +\n",
    "                  \"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "\n",
    "pred_prob = network.predict(x_test)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "print(\"Final Test Accuracy : \" + str(np.mean(pred_label == np.argmax(t_test, axis=1))))\n",
    "# print(\"Final Test Accuracy : \" + str(network.accuracy(x_test, t_test)))\n",
    "# .accuracy() メソッド使わず手で算出．ぴったりの epoch で終わってないからズレてて良い．\n",
    "\n",
    "\n",
    "# backward propagation ではなくナイーブに数値微分で勾配を求めたときは，\n",
    "# batch_size = 100, iters_num = 100 (つまり 1/6 epoch)　で 2 時間くらいかかってた．\n",
    "\n",
    "# 最後の方で train / test で acc に差が出てきている．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5gT5drA4d+zld470ovSBQERwS5FQVQExIIKinrEw/HTY8GCBcVyVCyoiFhQARWUJogVCyodREEUBAQB6Uhny/v9MZnsJJm03c0mu/vc15WLaZl5M2TnydvFGINSSikVqaR4J0AppVThooFDKaVUVDRwKKWUiooGDqWUUlHRwKGUUioqGjiUUkpFRQOHKhJEZK6IXBPvdMSDiIwWkf/EOx02EflQRHrEOx0qdkT7cai8EJGNwPXGmM/jnZZYEZFywMPApUAlYDswGxhljNkV57RVBVYAjY0xR+KZFpuIdAReNsacEu+0qNjQHIdKeCKSEsdrpwFfAC2AHkA5oDOwG+iYi/Pl92e5FpiTKEEDwBizCCgnIu3jnRYVGxo4VMyISC8RWSEi+0TkexFp7dh3t4isF5EDIrJaRC5x7LtWRBaIyLMisgd40LPtOxH5n4jsFZENItLT8Z75InK94/2hjm0gIt94rv25iIwVkXeCfIxBQF3gEmPMamNMtjFmhzHmEWPMHM/5jIg0dpz/TREZ5Vk+S0S2iMhdIrIdeENE1ohIL8fxKSKyS0TaedY7ee7XPhFZKSJnhbjNPYGvHeeaJSIHHa9sEbnWs6+ziCwWkf2efzs73ldLRGaKyB4RWSciNzj2PSgiH4jIO557tkpEmorIPSKyQ0Q2i0g3v3TNBy4MkW5ViGngUDHheQi+DtwIVAbGATNFJN1zyHqgK1AeeAh4R0RqOk5xKvAHUA141LFtLVAFeBKYICISJAmhjp0ELPKk60Hg6hAf5TzgE2PMwfCfOqgaWEVc9YChwGRgoGN/d2CXMWaZiNQGPgZGed5zBzDNUyTlphXW5wTAGNPbGFPGGFMGuAyrWO0LEankOe/zWJ/7GeBjEanseetkYAtQy/O+x0TkXMd1egNvAxWB5cA8rOdHbaxivHF+6VoDtAl/a1RhpIFDxcoNwDhjzEJjTJYx5i3gGNAJwBjzgTFmq+cX/HvA7/gW/Ww1xrxgjMl0FMNsMsaMN8ZkAW8BNYHqQa7veqyI1AU6AA8YY44bY74DZob4HJWBbbm6AzmygZHGmGOezzIJuEhESnn2X+HZBnAVVtHTHM+9+QxYAlwQ5NwVgAP+G0WkKTARGGCM2Yz16/93Y8zbnns6GfgV6C0idYAuwF3GmKPGmBXAa/gG1G+NMfOMMZnAB0BV4HFjTAYwBagvIhUcxx/wpE0VQRo4VKzUA273FLfsE5F9QB2sX7SIyCBHMdY+oCVW7sC22eWc2+0FY8xhz2KZINcPdmwtYI9jW7Br2XZjBZ282GmMOepIzzqsX+S9PcHjInICRz2gn9996xIiDXuBss4NIlIemAHcb4z51rO5FrDJ772bsHIM9j054LLP9rdj+QhWDinLsQ6+/xdlgX1B0qwKOQ0cKlY2A48aYyo4XqWMMZNFpB4wHhgGVDbGVAB+BpzFTrFq7rcNqOT4tQ9WQAvmc6C7iJQOccxhwHm+Gn773T6LXVzVB1jtCSZg3be3/e5baWPM40Gu/RPQ1F4RkSSsIPSVMcZZfLQVKyg51QX+8uyrJCJlXfblVjNgZR7erxKYBg6VH1JFpITjlYIVGG4SkVPFUlpELvQ8nEpjPUx3AojIdVg5jpgzxmzCKvp5UETSROQ0rPL7YN7GephPE5GTRCRJRCqLyAgRsYuPVgBXiEiyWP0XzowgKVOAbsDN5OQ2AN7Byol095yvhKeC/YQg55njd71Hse7vcJfjmorIFZ7K+AFAc2C2pyjre2C053qtgSHAuxF8jmDOBObm4f0qgWngUPlhDlZxhf160BizBKue40Ws4pR1WE1HMcasBp4GfsAqAmkFLCjA9F4JnIZVDDUKeA+r/iWAMeYYVgX5r8BnwD9YFetVgIWew4ZjBZ99nnNPD5cAY8w2rM/f2XN9e/tmrFzICKzAuhn4L8H/VicCF4hISc/6QKx6pL2OllVXGmN2A72A2z2f+06gl6MfykCgPlbu4yOsOpnPwn0ONyLSATjkaZariiDtAKiKPRF5D/jVGDMy3mnJDRF5DNhhjBkT77QAiMg0YILdXFkVPRo4VLHj+UW8B9iAVVw0HTjNGLM8rglTqpCIW49cpeKoBvAhVlPbLcDNGjSUipzmOJRSSkVFK8eVUkpFpdAVVVWpUsXUr18/3slQSqlCZenSpbuMMcGGrolKoQsc9evXZ8mSJfFOhlJKFSoi4j9yQK5pUZVSSqmoaOBQSikVFQ0cSimloqKBQymlVFQ0cCillIqKBg6llFJR0cChlFIqKoWuH8e2bQeZOHElR45kcMYZ9TjppCoEn3ZaKaVUfit0Y1WJ1DJwo3c9NTWJzp3r0KVLXXr0aEyXLnXjmDqllEpMIrLUGNM+X85V2AJHu3anmAkTZvPee79QokQK33yzia++2ujdX7NmGS64oAmvvXZR/BKplFIJJj8DR6ErqkpKEtq2rUnbtjUByM42HDhwjOPHs7juuhnMn7+RCROW07JlNYYOPYVSpVLjnGKllCpaCl2Oo3379ibUWFUbN+6jQYPnvOtz5lxBz55NCiJpSimVsPIzx1HkWlXVr1+BCRNyiqkGDZrOH3/sjWOKlFKqaClygQNg8OC2GDOSSy9txq5dh2nU6HnWr98T72QppVSRUCQDh23atP7e5REjvoxjSpRSqugo0oEDoFWraoDVbFcppVTeFfmn6axZAwF4991VbNiwl927D8c5RUopVbgV+cBRr14F7ruvKwANGz5PlSpPxTlFSilVuBX5wAEwfHineCdBKaWKjGIROCpXLklyso5npZRS+aFYBA4RISsrp6PjG28sj2NqlFKqcCsWgQOgb99m3uXBg2fGMSVKKVW4FZvAcdttWs+hlFL5odgEjtNPr8tHHw3wrh86dDyOqVFKqcKr2AQOgIsvPokHHjgDgMaNX4hzapRSqnAqVoEDoFGjSgBs336Qgwc116GUUtEqdoGjTZvq3uVhw+bEMSVKKVU4FcPAUcO7vHbt7jimRCmlCqdiFzgAGjSoAEBWVnacU6KUUoVPsQwcmZlWwFi8eCt16z7LsWOZcU6RUkoVHsUycGRk5OQ0Nm/+h23bDsYxNUopVbjELHCISB0R+UpE1ojILyIy3OUYEZHnRWSdiPwkIu1ilR6n++8/w2ddcxxKKRW5WOY4MoHbjTHNgE7ALSLS3O+YnkATz2so8HIM0+P1r391YOPGnDi2atWOgrisUkoVCTELHMaYbcaYZZ7lA8AaoLbfYX2AicbyI1BBRGrGKk1OdeqU9y736/dBQVxSKaWKhAKp4xCR+kBbYKHfrtrAZsf6FgKDCyIyVESWiMiSnTt35kuakpJ0mHWllMqNmAcOESkDTAP+Y4z5x3+3y1tMwAZjXjXGtDfGtK9atWoskkl2dsBllVJKuYhp4BCRVKyg8a4x5kOXQ7YAdRzrJwBbY5kmp6FDc+ri9+w5UlCXVUqpQi2WraoEmACsMcY8E+SwmcAgT+uqTsB+Y8y2WKXJ37hxvZk48WIA9u07WlCXVUqpQi0lhuc+HbgaWCUiKzzbRgB1AYwxrwBzgAuAdcBh4LoYpsdV+fIlAA0cSikVqZgFDmPMd7jXYTiPMcAtsUpDJCpUsAJHhw7jee+9y+jfv0U8k6OUUgmvWPYcd7IDB8Do0d/FMSVKKVU4FPvAUbFiTuDQllVKKRVesQ8czo6AGjiUUiq8Yh84AOrVs4KHBg6llApPA4eDVVevlFIqFA0cgNXlBNas2RXnlCilVOLTwOFn9+7D8U6CUkolNA0cfg4ePB7vJCilVELTwOFn8eICGypLKaUKJQ0cgDj6t/fr94G2rlJKqRA0cADVqpX2Wf/rL//R35VSStk0cAAffjiAa65p413fvVuHWFdKqWA0cAC1apXlv//t7F3XllVKKRWcBg4PZ72G5jiUUio4DRweJ55YhZYtqwGwa5fmOJRSKhgNHB5packsWzYU0KIqpZQKRQOHQ2pqMuXKpWtRlVJKhaCBw0+VKqW0qEoppULQwOGncuWS7NypgUMppYLRwOGnbdsafPfdnxw+nBHvpCilVELSwOGnW7dGHD6cwerVO+OdFKWUSkgaOPzYTXJ/+WVHnFOilFKJKWzgEJHhIlJOLBNEZJmIdCuIxMVDgwYVSUoS/vhjb7yTopRSCSmSHMdgY8w/QDegKnAd8HhMUxVHaWnJ1KpVlo0b98c7KUoplZAiCRz2oOMXAG8YY1Y6thVJ9eqV59dfdRpZpZRyE0ngWCoin2IFjnkiUhbIjm2y4is9PYVFi/7itts+iXdSlFIq4UQSOIYAdwMdjDGHgVSs4qoiy26KO2bMwjinRCmlEk8kgeM0YK0xZp+IXAXcBxTpCoCsrCKdoVJKqTyJJHC8DBwWkTbAncAmYGJMUxVnmZkaOJRSKphIAkemMcYAfYDnjDHPAWVjm6z4ysrSOceVUiqYSALHARG5B7ga+FhEkrHqOYqsp54637u8ceO+OKZEKaUSTySBYwBwDKs/x3agNvBUTFMVZ926NaJ//xYAXHXVh3FOjVJKJZawgcMTLN4FyotIL+CoMaZI13EAdO1aF4Djx7PinBKllEoskQw50h9YBPQD+gMLReSyWCcs3v71rw60bVuDlBQdzksppZwieSrei9WH4xpjzCCgI3B/uDeJyOsiskNEfg6y/ywR2S8iKzyvB6JLemwlJQm1apXVHIdSSvlJieCYJGOMc6jY3UQWcN4EXiR0091vjTG9IjhXXKSnp3DsmAYOpZRyiiRwfCIi84DJnvUBwNxwbzLGfCMi9XOftPhLS0vWHIdSSvkJGziMMf8VkUuBLliDG75qjPkon65/moisBLYCdxhjfnE7SESGAkMB6tatm0+XDi89PZljxzIL7HpKKVUYRJLjwBjzIeBtlyoiC4wxp+fx2suAesaYgyJyATAdaBLk+q8CrwK0b9++wHrnaY5DKaUC5bbJUJ5/9htj/jHGHPQszwFSRaRKXs+bn9LSkrWOQyml/OQ2cOT5V7+I1BAR8Sx39KRld17Pm5/S0zXHoZRS/oIWVXnqNVx3ASXDnVhEJgNnAVVEZAswEs9QJcaYV4DLgJtFJBM4AlzuGRMrYaSlJXPw4HFEHuKJJ87jzjvzWjqnlFKFX6g6jt4h9s0Od2JjzMAw+1/Eaq6bsNLTc27P2LGLNXAopRQhAocxpkhP1hSJtLTkeCdBKaUSjo6nEUJqas7tkSI9y7pSSkVOA0cIf/11wLssGjmUUgrQwBFSmzbV450EpZRKOBEFDhHpLCJXiMgg+xXrhCWCa689maZNKwNaVKWUUrawPcdF5G2gEbACsDs1GIr4vONgFU+VLZvmXVZKKRXZkCPtgeaJ1seioNgB4+DB47z77k9ceWXrOKdIKaXiK5Kiqp+BGrFOSKKy4+WOHYe46qqP+OWXHWHeoZRSRVuonuOzsIqkygKrRWQR1tzjABhjLop98hJPUpIWWSmlirdQRVX/K7BUJDD/uo3s7GJZYqeUUl5Bi6qMMV8bY74G/gQWOtYXAZsKKoHxduutHX3WddBDpVRxF0kdxwdAtmM9y7OtWBg0qA0XXpgzTYgGDqVUcRdJ4Egxxhy3VzzLabFLUuJp0KCCd/mGG2aRmZkd4millCraIgkcO0XEWxEuIn2AXbFLUuJp3LiSd3nVqh38+OOWOKZGKaXiK5J+HDcB74qIPQT6FuDq2CUp8Zx7bkOf9eRkbVmllCq+IslxZBtjOgHNgRbGmM741nkUeS1bVmP8+JzpSZKTdYgvpVTxFckTcBqAMeagMcYeLnZq7JKUmDp1OsG7rDkOpVRxFqoD4ElAC6C83zSy5YASsU5YonHOzaGUUsVZqDqOE4FeQAV8p5E9ANwQy0QlIudsgFlZ2glQKVV8hZo6dgYwQ0ROM8b8UIBpSkjOwKHNcZVSxVkkraqWi8gtWMVW3iIqY8zgmKUqATkDR0aGdgJUShVfkRTcv401Om534GvgBKziqmJFcxxKKWWJJHA0NsbcDxwyxrwFXAi0im2yEk+ZMjmd5TVwKKWKs0gCR4bn330i0hIoD9SPWYoSlLPvRkaGBg6lVPEVSeB4VUQqAvcDM4HVwBMxTVWC6917Mp98si7eyVBKqbgIGziMMa8ZY/Z6hlVvaIypZowZVxCJSzTPPtvdu3zDDbPimBKllIqfsIFDRCqLyAsiskxElorIGBGpXBCJSzQ9ejT2LlesWOz6QCqlFBBZUdUUYAfQF7gMa2Tc92KZqETl7D1eokQkLZmVUqroieTpV8kY84hjfZSIXByrBCWylJScwNGsWdU4pkQppeInkhzHVyJyuYgkeV79gY9jnbBE5Awc3333Jyed9CL//HMsjilSSqmCF2qQwwOAAQT4P6yOgADJwEFgZMxTl2CcgeOPP/YCsGTJVs45p0G8kqSUUgUu1FhVZQsyIYWBM3DY0tOTXY5USqmiS8cKz6PUVA0cSqniRQNHFCpUKEGHDrW4++7Tvdv69n0/jilSSqmCF7PAISKvi8gOEfk5yH4RkedFZJ2I/CQi7WKVlvySnJzEokU3MHr0ed5tW7b8E8cUKaVUwYukA2All1dqBOd+E+gRYn9PoInnNRR4OZIEK6WUiq9IchzLgJ3Ab8DvnuUNnp7kpwR7kzHmG2BPiPP2ASYay49ABRGpGXnS4+uMM+p5l0eN+iaOKVFKqYIVSeD4BLjAGFPFGFMZK6fwPvAv4KU8XLs2sNmxvsWzLYCIDBWRJSKyZOfOnXm4ZP4ZO/YC77IGDqVUcRJJ4GhvjJlnrxhjPgXO8OQS0vNwbXHZ5jqZtzHmVWNMe2NM+6pVE6PHtnNip+xsnYNcKVV8RDLkyB4RuQtrzCqAAcBeEUkG8jIxxRagjmP9BGBrHs5XoDRwKKWKq0hyHFdgPdSnAzOAup5tyUD/PFx7JjDI07qqE7DfGLMtD+crUM6Of1lZOYEjO9uwdGmhiX9KKRW1sDkOY8wu4NYgu4POZiQik4GzgCoisgVriJJUzzlfAeYAF3jOcRi4LpqEx5szx+H01FMLuPvuL1iwYDCdO9dxPUYppQqzsIFDRJoCd2BNF+s93hhzTqj3GWMGhtlvgFsiSmUCcusxvnz5Np588nsANm/ej29JnFJKFQ2R1HF8ALwCvAZkxTY5hUepUr5dWWrXfoatWw/EKTVKKVVwIgkcmcYY7ZznJyUlib59mzFt2hoADRpKqWIjksrxWSLyLxGp6ew9HvOUFQJjx15Ay5bVXPft3XuUu+76jIwMzaQppYoWsaoaQhwgssFlszHGNIxNkkJr3769WbJkSTwu7WrnzkNUq/a/gO2lSqVy+HAGkyZdysCBreKQMqWUyiEiS40x7fPjXJG0qtJZikKoWrW06/bDhzMAyMzMS1cXpZRKPEGLqkTkHM+/l7q9Ci6Jie+hh86KdxKUUqrAhKrjONPzb2+XV68Yp6tQGTKkbbyToJRSBSbU1LEjPf8Wqo558VCmTFrQfd9/v5mrr25TgKlRSqnYiqQDYDrQl8AOgA/HLlmFS+nSwQPHK68s5ZJLmtGtW6MCTJFSSsVOJM1xZ2DNnZEJHHK8lEdKShIbNgznqafOd92/du2uAk6RUkrFTiQdAE8wxoSayU8B9etXoFw591HmDx3KKODUKKVU7ESS4/heRLQjQgScI+Y6HTx4vIBTopRSsRNJ4OgCLBWRtSLyk4isEpGfYp2wwqhECfcM3KFDoQPHAw98xeef/xGLJCmlVL6LpKiqZ8xTUUSUL1/CdfuRI5ne5ZdeWszpp9ehTZsa3m2PPGJNPetpyKaUUgktaOAQkXLGmH8AHb0vQpUrl3Td7uw9fsstcwANEkqpwitUjmMSVke/pVhzgTvnCDdAXMaqSmSVKrkHjowMK3Do8CNKqaIgVAfAXp5/dayqCFWuXMp1+9Spq2nVqho33nhKwL5wg0wqpVSiiaRyHBGpKCIdReQM+xXrhBVG5cu7N8c9fDiD//73M+/AhwDbtx/kr7/+ITs7J3BkZmbz0kuLdSh2pVRCi6Tn+PXAcOAEYAXQCfgBCDl1bHEkIhgzkrZtx7FixfaA/TVqPO1drlnTWl68+AbvttTURwA4cOAYd93VJcapVUqp3IkkxzEc6ABsMsacDbQFdsY0VYVcsLoONx06jA/Ytn793vxMjlJK5atIAsdRY8xRsMatMsb8CpwY22QVbu3b18zT+/fsOZJPKVFKqfwXSeDYIiIVgOnAZyIyA9ga22QVbqNGncOsWQNz/f7duzVw+BN5iG7d3o53MpRSRDYD4CWexQdF5CugPPBJTFNVyKWmJtOrV1M+/fQqvvlmE6NGfRvV+w8cOBajlBVun32mveuVSgQhcxwikiQiP9vrxpivjTEzjTE6+FIEzj+/EQ8/fHbU73MbFHHduj1Mnbo6P5IFWC29Fi7ckm/nU0oVHyEDhzEmG1gpInULKD1FjoiwZcttdOli3cLHHgvfGM1tbKtmzcbSr98H+ZauQYM+olOnCezcqSPkK6WiE0kdR03gFxH5QkRm2q9YJ6woqV27HBUrWuNYVani3knQyS3H4d/rfPr0XznvvIneDoQVKz5Bz57vRpymhQv/AnzH0VJKqUhEMsjhQzFPRTFgzxKYmuo+9LpTqNF0jTGICJdd9j5ZWYbTT3+d778fwr59R/nkk3U89ti33Hvvl2RlPUBWVjZ79hyhevUyrucBEAnYpZRSIUWS47jAU7fhfQEXxDphRY09AGJSUvgn9bFjwXuO2z3N69evAMAPP2zhyJGcHMrDD38NwNGjmdx442xq1Hia48cDz2ePdOLsue5v8eK/WLDgz7DpVUoVL5EEDrf5UHWo9SiNGnUO//d/nRgwoEVEx9tFU3v2HOHnn3cEbC9VKtW7bdeuw97ltDQrR3PkSAaTJ1vtGo4eDSyOsnMcoQZe7NjxNbp0eSOi9Cqlio+ggUNEbhaRVcCJngmc7NcGQCdyilKFCiV4+unupKen8NJL4TNsHTuO5+OPf6Ny5Sdp1epl73Z7pF1nTsEZOOyisCNHMsnKso51y3HY77fPF41p01YzcuRXDB06iz17jvD88wt1sEalipFww6rPBUYDdzu2HzDG7Ilpqoq4G244hX/9a07IY5Yv306vXpMDtm/YsJdWraqTlZXzoG7X7lXvst3r/Nprp3uPCVVUlZsBFS+7LKd117Rpa9iz5wjNm1flvPN0pH2lioNQw6rvB/YDue8CrVylpEQ0KLGr1q1f4e67Tw9ZNwHwxRcbvMv+gWP9+j3s2GE1w81NjsPJDlTh0qOUKjpy/wRT+aZr17rs2XNnxMePG7c0qkmhdu48xLffbvKud+/+jnc5MzOb7dsP5vnBH2y+daVU0aOBI04mTbrUu/zFF4N85iuvW7d8yPceOZLJwYORd97v0eNdzjjjTW8z3507c+pENm/eT82aT/Pgg/MxxvDAA1+xYUP0o/OmpupXSaniIqZ/7SLSQ0TWisg6EbnbZf+1IrJTRFZ4XtfHMj2JZODAVsyYcTn33NOF1NRkn2a6N9/cPuR7jx7NjGo8K7s4ye7s52y+u2nTfgBmzlzL77/v4ZFHvuHSS98PuF44Bw8e559/dIwtpYqDmJUviEgyMBarOe8WYLGIzDTG+A+49J4xZlis0pHILrroRC66KHCE+khaKOWmx7cdMJz1GseOWedJTk7yVpTb22ylSz9GVtYDIc/drZtV/GXMyKjTpZQqXGKZ4+gIrDPG/OEZFHEK0CeG1ysynJXn1auXDnpc1arhhy9xOnIkM6Auw84lJCWJz76tWw94l7OzDdnZhrVrd7FyZeDMhpE4fjzLZ+rcaOS1qW9mZjY33DCTP/6I/wRZzzzzAz/8sDneyVAqT2IZOGoDzr+QLZ5t/vp6+odMFZE6MUxPoXHzzR28y+XKuc9jDvDoo9HN3nvkSAYjR37ls+3xxxd497Vu/QoAa9bsonbtZ3yOS05+mJNOGsvJJ4+L6pq2rl3foHTpx4LuX7ZsG6+/vtx1X14r7hct+ovXXlvOVVd9mKfz5Ifbb/+Uzp1fj3cyfCxdupVFi/6KdzJUDOX3VA2xDBxuY2v4PwFmAfWNMa2Bz4G3XE8kMlRElojIkp07i/astZdd1pwyZdLYv9+qErKHFnFTtmzwoOJmy5Z/mDDB/eH8yy/5d19FHuK223ynbAn3YDrllFcZMiRn7MxjxzIZMeILDh487tNnJTfs3vShhnJxmj79V/buLT6TabVvP55TT30t3slQuTRv3rqw9ZDlyj2er9eMZeDYAjhzECfgN3OgMWa3McYOheOBU9xOZIx51RjT3hjTvmrVqjFJbCLIynqA9967DLByGr/9NowpUy4Lenyo3IibXr0ms23bwTylMRy7t/qYMQtd9zuLqxYt+guRh1yDyoQJyxk9+jseffSbPOc47BZfbh0h/e3de4RLLnmPCy+clKdrJprjx7Mi+vy2GTN+ReShIj3s/q+/7orqniSipUu30qPHu9x++7wCvW4sA8dioImINBCRNOBywGc4dhFxTs59EbAmhulJeElJ4tO6qkmTylSqVJJDh0Ywe3ZgP8xKlUoWZPIiEu5XvbO46rPP1gPw4Yc5/+3+FfRu9TK2FSu2s3HjvrBpst/vX+nvZIzh6NFMb/p/+CH/J7ly+xxr1uz0+fyxUrbsaOrUeTbi45999kcAn3HSbL17T6ZGjf+FfP/vv+9m/vyNUaUxlowxPPnkAm/d3Y4dh2jWbCy33PJxnFOWN3aLybVrdxfodWMWOIwxmcAwYB5WQHjfGPOLiDwsIhd5Dvu3iPwiIiuBfwPXxio9hVmpUqk+gxraKlUqyV13nR6HFAXnzDLv23cUCF5HYfddcTbjtXMk4hnv3RjjzcUAjBr1jbcYqW3bcTRo8FzYNNmtyI4fz+Lo0UyuvXY6a9ZYRXMLFvzJggV/8vTTP1Cy5I2359MAACAASURBVKM+jQJuueVj5sz5nU2bwgenSLgN79K8+Uv07fu+y9H56/jxLO9oAZGw2yOIy7j7s2f/xt9/hz5X06YvcvbZriXPcbFhwz7uuutz+vSZAsD+/dZ3c/78TaHelvDsH5oFPXJDTPtxGGPmGGOaGmMaGWMe9Wx7wBgz07N8jzGmhTGmjTHmbGPMr7FMT2HmNkxJpUolefzx8+jY0a3NQeQ6dTohT+93cv6qv+OOTzl06Dhbtvzjc4xdPFCmjDVHyf79OYHDnsTKfl4Z4/tHcf/9X1Gp0pM89ND8sGnZsGEvQ4bMYNWqvz1py2Ly5FW89dZKnnzyewC6dHmDLl3e4NVXlwK+rcleemkJF144ifr1wwcnN2vX7vIJiqGGd4nVIJHGGF5+eXHA9oyMLFavLrj6wh07DnHhhZPYvftw+INDOH48i+3boy9utf9+1q8vWsPs5fzAKtjranffQsItcFSoYP1i/+234NlU/4EH09KS+eyzq322Pfdcj3xIocX54H399eWUKTOaevXG+BxjZ6/tAOKcuGrv3iM89ti33pzLCy8scv019eCDX4dNS9eub/D66yu4/vpZ3uutW2c9OOrX9+2d//vv1nZn7iavTjppLOedN9G7HmpAyViVta9c+bfrgJq33jqXFi1eYtu2Ay7vyv+Jvp599gfmzPmdceOW5uk811wznZo1n476F7b9/7p371Gf7W4BOyMjy2f70qVbY9a5ddOmfZQoMYpffgksEoxEzg+sIpTjUPmnUaNKPutt29bwBpO33rrYdayoc85pwNy5V/Lbbzn9K9etu5XzzmvIjh13cPTovfz22zCaNq2cb+ls3368dznYdzmnJ3tgv45HHvmGe+/9kvHjl3m3RdqqatOmfezYcQhjDPPnb+Svv3wfiocPZ3h/oT388Dc89dSCgHNEM5RLKPYf8uLFW9m16zC//747ZHBYvjynf8zEiSu944llZmYzevS3uU5XsAfKvHlW/VKwB6K93a2oyvbxx7/F/IFVo8b/GDx4BmPG/MiMGb8ybZrVfzja5qXhxnabMGEZW7ce4MiRDNLSRvHgg/MBKwfdvv14LrnkvYiv9eef+0PO4uk0depqjh3LCtraMRzNcaiQqlXL6QhozEiWLbvRu37RRSdy5Mi9PPaYb7+OlJQkUlKSqFMn59e13RKratXSpKen0KRJ5ahbZ+XV3Lm/Azn1Gc6irMWLrYZ3djEW4FoB7h8ojx3LpH795zj//Ld57rmFruXrzvqX7GzDnXd+HnCM23zvYDUxjqaIxNlIoGrVp2ja9MWQRVWnnTaB1at3MmnSKq65Zjqffmo92GfNWsuIEV9y112fRXxt33QHPvg//XS9t7WU857MnLmWPXuOcPhwBitX/u15P5x66mtUqvREwHl69ZrMr7/uiiodY8cuZuHC0A0P9u076i1e/PvvQ7zxxgpuu20eF1/8HiVLWnV9zuLNSPgHDvvHiIiwbdsBrr9+FhdfPMVbd/PmmysBHI0lIu+0Wa/eGM4//+2IjrVzTpHMDOomWFw/fDiD5s3H+gxump80cBQh9rzmtuRk61uVnp4zz7nzgWzL7Zc2t+64w3oI3nef1Rlx6dJt3n127277wQXQocN4/DVv7tss227989NPf4csugs3/0ioX7LLl28jMzObb74J/seYnW1YsmSra24q3LVbtHiJK6/07aQ4bZrV4mr16sAH9NGjmWzcuI+srGxGjPjCtemsf0uytWt30b37O94A+eef+737+vSZwvXXzwzI3Sxa9FdAEY/NOfTNvHnrGDt2ketx9gNu69YDdOo0wfUYW9eub9C69Suuv9rtHwx25bbTmjU7+frrja7n9A8cznU7eO7addib0/rzz/2MHbvIe/+cfyMjR37F0qU+PQs4cOAY9933pff/ONJWefaPl7z+Dfrn/H75ZQdr1uzijDPejEmTag0chcj69f9m6tR+QfeXLu3b8qp27bKA76/O5OTE+C+vWvWpPLUEsYu7bHaxVGpqUsiRetetCz3sSKgioePHs7jzzs8488w3GT58Lo888jUDBkxl6NBZPPDAV2RmZjNhwjI6dBjPRx8FtvPYvPkfl7MGt2jRX7z77irvtUeP/pZJk1Z5919xxTQaNHiOzz//g9Gjv+OmmwKblvp3DPPvx7N+ve/9cM4mCb7FhC++GBgUnMVvPXq8y7Bhc10/S6giL3/2jwC3PkclS1qBY9++o2zdesA7tbLIQzRv/hJnnfWWa9AOFjjWrdvjzflu2LCPJ57IKb4cNmyu9/45Wy89/PA3PkWyACNHzufRR79l4sSVYT+f/z12nn/ChGW0bv1ywP5g7PuanW0YO3YRtWo9HfBZL798WsTni5ROolCINGxYkYYNKwbdb+c46tQpx8iRZ3L55S0LKmlRc/vjiYaz+Cot7RFvBX+FCiVCBqRwLYlCBY6vv97k7d/w/POBD1GrfqYrgE8veNuZZ77pXd669QC1apUNmRZnb+5y5dIZMeJLAK64ohWANzgdOGCledu2A7z22jL69Wvubers36/GP0dlNxaw1apV1idn5HwI3XrrXIYN6+hzfKhpiZ1yU8n+999ugSOnqKp27WcoWTIlIKd98OBx73E2/4dp165veJcXLszpgOoMzJATeO0HdLC+QHawClbUaatXbwx//rmfjz++ggsuaOLdbpcOOBty2CMehGLfa2PwBu1Dh4771Hk4c5X5JTF+fqp8Yff1OHYsiyFD2gX8QYUya1bhnegxIyPb23KodOm0kCMHh+uTEarVjx00Qom0U2bt2s/wxBPfedfPOadByOOdrep6957Mm2+u8K7362dN5fvDD1u44YZZDBs2l3vu+dzbqdHJvzLcP3Ckp6f4DAbp/8D1f3C6BY7x4yNrOTV79m8h60j8c5WQM3yMHQCPHMkMaAnnNpim/+dw/kBwK/ay2fVado4gWAMHOyc/Z87vQc8FOQ9x/7oH+/x2AHH77GDd25kz1wLw3Xd/ev8/nEVV/o1J3HJgeaWBowixi6oimT/DX69eTb3LGRn3e5f96xIAXn21V9TnL6iJnjZu3BeyhUq44ejtX++5FU3rp7vv/sK7HKpXO+Azdtbs2b9x3XUzgh77zjs/8fjjC1i06K+AB3O4wLFp0z7OOONN77r/A7dEiUd91t0epJEWyfXuPZlmzcb6bEtOfti7vHt34MPTfsA69/n/nx46lMGkSauYPt3Kkb3//i8+A0t26eI7yKRb73hbz57vAlaO6aabZvPZZ3/47P/Pfz7h1FNf86bLbq3mxvl3mZmZ7fOwt99vjz+3e/dhfvrp74C6i6FDZ9OnzxSWLNlK165vcM891nfIedj27Qd9cqu5mYIhHC2qKkLsHEa4h1Aw48f3Jisr2+fXbeXKvr+gW7eunqsOh+npKWRk5E9T10SW24rIBQtCt9px9o+JlFsltH8lt38dx9df+/4SDlehn5GRxbZtB/jgg5xpdpyjHIwe/S1XX92GUaO+jSjNzmIut1/d9i9yZ67I/4fS4cMZ3kYGxozkrrt8W8/53+vXX19BMHbRU2ZmNuPGLfXJkT755AKee84ak61Vq2oB723V6mVWrbrZu+4M/pmZ2T6t7OzAUa5cOvv2HWXGjLXce++X9O/fgsmT+wZUntv9b5yNSGz2UD62YLmXvNDAUYTYOY727WsF7Hv22e5hm91ef327gG3+lelz516Zqz4F55zTwJvFdjrzzHreh1XJkikx+XWUGyK5axv/4ouBvbTzQ256S7sZOXJ+wLZ27WqybNm2wIPxbfHmZs+eI9Sq5TsEv/OHx4gRX+b6weU2QrGdnlB1ZM7e6bNn/5YvQ4q75eKdAcktl+ufk3Heh8zMbJ8feHZgsFs92gHh/fd/oV27Gtx1VxeGDcvpyGl/N3PqOHK+rHZn1ljSoqoipGXLarz88oV8/PEVAfv+859ODB7cNupzvvlmH4YPP9W7XqtWWW/LFlu7djW5/fbTmD//Gt5++xLX87Ro4T6q8WefXe2dVyQ1NXxlYEFx9n1JBPkVUN06wtWrF/yzPvpo6JzC4MGBjQD8f+HntvjPrajKFqx5MFitu2y9e08OeZ5IheqDEylnmq3AkZOb++OPfYg85G284cx5rVplBaCxY3N+lPg3QHCu58fnDUdzHEWIiHDTTaHnK4/UqlU3k5WVTb16FRgzpgf9+jX3ZtudLVa+/HIQrVtXp3LlnNkIr776o4DzBRvKIzU1mZo1y3iWfX/HVKxYwvUB8d//duapp76P6vPMmHG5d4C7cMqWTfMWifjr1asps2f/FtW1E52zc2mkmjWrwpo1kXX+8+/BD76/kHftOsxff/1Dq1bVfY4JlVOxB9AsDLKzDZs37w9oyVW16lPedf+pBZwV/G6t1PyHKHHmjgvi3miOQ7lq2bIabdrU8K6ffnpdunVrBOCT4zj77AY+QSOYUMOG2BWC/jmOlStvCji2T58TefLJ85kypa/rw/2FF3q6XsNtbnebf7PltLTkoP1d6tYtx4UXNnHdBzB0aGBxX6LLTeAINcGYP7cA4Mz59OjxDiefPC6gw9/OncGLo7777k+AoAE+Udx77xdcfPGUgIEynUPMAC6fPaeuzC1w+L9/yZKcDomffLIu1+mNlAYOFbVSpVK56qrWfPnloJDHXX11awAGDz7Zm+Po27cZAJde2sz7fruJZZUqOQHopptOoU6d8vTs2djnnHZwGTCgJT17Bj7A3epxzj67fsh0PvNMt4BrBHsglSyZSuvW1V33AZx0UhXX7S++6B7QwGq5Zufc8tvUqf248spWIY+Jdu56wHWY/2Dc6hicRT92vYVdJJOz3bd3thvndyYRPfbYd8yaFT6H6h8knetuP7piNehipDRwqKiJCG+/fQlnnx2678HEiZeQkXE/r712kffL37VrXYwZybRp/b3vP/30OvTv34Lp0wd43/vyy1aT348+GsD55+eM8OsszvJvn/7FF4NcH2huvZadD+maNcvyww9DvINB1q5dNmiOo1SpVB566Cy+/HIQl17aLGB/sHvin67XX7/Iu9yiRVXOPrsBgwef7Prexo0ruW6PRN++zcM2iqhaNSfHMX36gJB9UU4+2cqFRhc4Aus43OYG8e+cGcl4VKHqOgoT/74nzhzHli3/8PHHvsEnFi2loqGBQ8VUSkoSIuLNfbjlEipXLsV7710WMAIwWM14+/dv4V13FmfZQ1J88smVHD48gnPOaeB9oJUpk0azZu6//gF+/HGIz3qnTifQpEllXnihJzNmXO7NcZxySk2f40qVSiU1NZmzz27g2hv65JNruBZXpaf7Vided11bjh27j7feupgJE6wg4lZxvWfPnQFpCMWuL3IKV5xTo0bOe/r0OSlg6BqnDh2sFnt5zXHcemvg0CS5GU2gsE/9GoyzB/qPP26hV6/JPvujmZQrFjRwqHw3a9bAgDk+OnSojTEjczWE+5Ahbb3DM5QokRM47FF1W7eu7q2wt3MkHTvW9s47MmiQFbTOPLOe973+Q1LYhg3rSO3a5bw5jmef7e6z31ms4zZgJMC4cb0DtjkbB9hFR2lpyQwa1MZbx3P11W0C+siULJkadjpeJ2fdQ9myVvrsz3LrrR15+OGzAt5Tp045n3X/oPDf/3YGrM9uDzLo37IuFLdiFbcGBpFONTtlSt+Ir11URTvuWX7TwKHyXa9eTfn3v08Nf6CLSZMuZdmyoT7bRISpU/sxdGg7HnvsXO/2kSPPBKB69ZxfzHaRWHKyULt2OYwZyTXXWEVAn38+iP/973zmzr0ybDrsdvX+w7c7K5KfeaY7Z51V3/X9/mOKOQfse+edS13fU61aaRYuvJ527XJyGOnpyT5FTT//nNOhbNCgNj7vN2akT+5h6VLrPtr9KqpWLcX995/p857y5dO9uSH7M/sHjocfPhuAbt0aeUdaTktLpmHDitx2WyfXz+LkrM9o0iR4sZvd63rSpMD748w1nX9+I+/yGWfU8zkuVCOIRDVmTPfwByUYDRwqoQwc2Iq2bQOLZkqWTGXcuN4+Lbj+7/9Ow5iRPr1q7eIjt4mtUlKSuP32zvTo0Thgnz/7QeXfoqVixZzy/0qVSjJunPvwKytW3Mj27bd719u0sepUvvgidIMCsB74drAQEZ5+Oqfy3jkwotskSvbgho88cjZNmli5u4EDrVZjHToE9vhPSUnyfla7orllS6sX9NCh7diwYTglSqSwceNwJky4yHtfk5OTWL/+39x6a8eAcwI8+OCZrtsXL74h2Mf2ql+/AtWr+7b0cv5/OnM7/v2GundvFHGLL7ciuWnT+od9nz3qdDTsolo3nTvXifp84fg3KslvGjhUkXLOOQ249daOvPJK+PG06tYtH7TozC7e8W/R4l9R7f+As5Utm+6TE+revTG7d98ZdjBD208/3eTtyOlsOeSsK3Hr2W4/VJ0PxVNOqUVW1gOuATM1NZnq1ctw442neIv2XnmlFx9+2J9x43p7H8L16lUgPT0loK4mWF1HsAr58uVLuE6D7HTqqSd4h8+xi+6cn9WZhooVS/i8d8+eI9StG77z5s6d/2X//rsDtjuLIu1GGa+8ciHXX5/TedYayv3mgPc63+Nv4kT3jrEQWP/lFKpzZjB16pRjzpwr2bHjDm8xY37TwKGKlNTUZJ5/vmfYIcsBNm4cztq1w1z3vf76RVx2WXPat69Fjx6NadmyGtnZD/gUBYH1gLQrjEeM6BLyepGOnAvWg9o57LbNOSmXW/t+uyjIv7VRsImCUlOTSEoSXnmll7elWalSqVxySWCLMec1gxVr2UL1tHZOZexv/vxrSEoS5s69knfeuYSvvrqGjRuH+zQccH4W/4duo0YVQ85ZY6tSpZRryznn57EbYtStW54XXriAKVP60rdvM2bOHEiLFoFjU73xRh8++KAfw4efGjLnYo+UkPMZkvnpp5t4553A4OLfvPuhh85iyJDQI0DkFE2WDpnTyQvtOa6KrVCTCzVrVpUPPrAeQKHqRESERYuCF7+sWHEjmzblfT6E6tVL8/ffh0hOTuKxx85hxIgvfQLH33/fAcAtt3Rkz54j3HxzZCMIRDvMi13RbecogjUycBsc0b6PDRpUpG3bGixfvp2ZMy+nS5e6TJy4kubNq3LmmfUBaNq0sjc3WK9ehYAWZ9Om9eepp773aZ69e/edVKxYAhGhXr3y3vv+4os9GTZsLtWqlQ7bGskZOF58sSf331+Cs89uQIkSKQwY0JIBA3I6i558cg1WrLA64r36ai+uvdaqSxszpod36luwRi2wP/8PP2zmkktO4t57v/TuT09P4cQTq7gGsvfeu4wKFXKm7R0ypG3Y+cmd/6fRTK0QDQ0cSsVQmzY1fHrg59bixTd4ewvffHMH3n9/NQ88cAZTpvwM5FTap6Ul88gj5wQ9j79oW7nZc1fYgSMlJYn09GQeffQcrriiFRs27GPDhr2uA+05i8pq1izL8uXbOXo0k4oVSzJ8eOhKdv/c1aWXNgvoR+PM0TmPt0c8KFEixdsB1V+jRhVZv34v5cql07t3UzZu3EeDBhWDNmQAq0n36tU7GTRoekAOzc6RDB58srfCvkePxvTo0Thg/g87F9m8eVW2b7+dypVLkZr6CJBTZwUwe/ZAatcu520F17hxpYBh8cF3kMmGDSsyb95VVK9empNPfjDoZ4mWBg6lCoE6dcp7B16sUKEEy5ffCFhD4fsXn4XTsGFFkpOFxx47l3PPjazOxXb4sDXYYvnyOXUYR4/e512uWbMsnTvX4d57rXki+vVr7jPkuu288xowZ87vrn1XQunevVH4g7Ae0uPHLwNy6qlKlkxh6lT3IqS5c69kzZpd1K5djpkzI5vULD09hbZta/oMnW5LShIOHLjHtZFGuXLplCqV6u305yxuc9aL2Z0twRpI9MILrTlz+vZtzuTJP/Piixdw771fMnWq7/31r0OyA2d+0sChVCHmNhR+OOvX/zvX13v88XNJSUkKWgdis+s4TjmlJu3b1+Krrzb67B8+vBP161egT5+TIrru88/34L77vuKTT66K6PgXX7zAGzjsVmN2KzM3TZpUDrk/N4L18xER7ruvq3cqYGe9le3AgXu8xXCHDo3wCQblyqXz6adWQ4a3376E0aPPpUmTF7z7C2LSNK0cV0pFrE6d8rz11sWuv6Sd7DqO1NRk7rzz9IB6oqQk4ZJLmgWttPd3662nuraCArj33q4BY4GlpSXz449DWLZsKE2aVGbSpEuDDvkfD/fc09W77NaqqkyZNO/2UqVSg84/XqJECo0bV+LccxvQoIHVAi5cq7X8oDkOpVS+O+20OowZszCq4VJya9Qo9zqdU089wbs8cGDogR7jKT8e9J9/PogNG/bSsOHzEfVTyisNHEqpfNe/fwtOP70OtWuXC3+wyhcNGlRk06b/cMIJsb/nWlSllIqJwhA0Fi++ge+/HxyXa19xRf7ngurWLR9x8V9eiNuwBYmsffv2ZsmSJfFOhlJK5Ul2tsEYE3QI//wmIkuNMfkyRagWVSmlVBxYOYPEnsEwGC2qUkopFRUNHEoppaKigUMppVRUNHAopZSKSkwDh4j0EJG1IrJORAK6fYpIuoi859m/UETqxzI9Siml8i5mgUNEkoGxQE+gOTBQRJr7HTYE2GuMaQw8CzyBUkqphBbLHEdHYJ0x5g9jzHFgCtDH75g+wFue5anAuRJqkgSllFJxF8vAURvY7Fjf4tnmeowxJhPYDwQMUSkiQ0VkiYgs2blzZ4ySq5RSKhKxDBxuOQf/buqRHIMx5lVjTHtjTPuqVavmS+KUUkrlTiwDxxagjmP9BGBrsGNEJAUoDwROaaWUUiphxDJwLAaaiEgDEUkDLgdm+h0zE7jGs3wZ8KUpbINnKaVUMROzsaqMMZkiMgyYByQDrxtjfhGRh4ElxpiZwATgbRFZh5XTuDxW6VFKKZU/YjrIoTFmDjDHb9sDjuWjQL9YpkEppVT+0p7jSimloqKBQymlVFQ0cCillIpKoZsBUER2ApvinY4EUQXYFe9EJAi9Fzn0XuTQe5HjRGNM2fw4UaGbAdAYoz0APURkSX5NBVnY6b3Iofcih96LHCKSb3Nua1GVUkqpqGjgUEopFRUNHIXbq/FOQALRe5FD70UOvRc58u1eFLrKcaWUUvGlOQ6llFJR0cChlFIqKho4EpSI1BGRr0RkjYj8IiLDPdsrichnIvK759+Knu0iIs975m//SUTaxfcT5D8RSRaR5SIy27PewDNX/e+euevTPNuL9Fz2IlJBRKaKyK+e78dpxfV7ISK3ef4+fhaRySJSojh9L0TkdRHZISI/O7ZF/V0QkWs8x/8uIte4XctJA0fiygRuN8Y0AzoBt3jmbL8b+MIY0wT4wrMO1tzuTTyvocDLBZ/kmBsOrHGsPwE867kXe7HmsIeiP5f9c8AnxpiTgDZY96TYfS9EpDbwb6C9MaYl1ijcl1O8vhdvAj38tkX1XRCRSsBI4FSsKb9H2sEmKGOMvgrBC5gBnA+sBWp6ttUE1nqWxwEDHcd7jysKL6yJwL4AzgFmY80euQtI8ew/DZjnWZ4HnOZZTvEcJ/H+DPl0H8oBG/w/T3H8XpAz9XQlz//zbKB7cfteAPWBn3P7XQAGAuMc232Oc3tpjqMQ8GSp2wILgerGmG0Ann+reQ6LZI73wmwMcCeQ7VmvDOwz1lz14Pt5I5rLvpBqCOwE3vAU270mIqUpht8LY8xfwP+AP4FtWP/PSyme3wunaL8LUX9HNHAkOBEpA0wD/mOM+SfUoS7bikRbaxHpBewwxix1bnY51ESwr7BLAdoBLxtj2gKHyCmKcFNk74WnOKUP0ACoBZTGKo7xVxy+F5EI9vmjvi8aOBKYiKRiBY13jTEfejb/LSI1PftrAjs82yOZ472wOh24SEQ2AlOwiqvGABU8c9WD7+ctynPZbwG2GGMWetanYgWS4vi9OA/YYIzZaYzJAD4EOlM8vxdO0X4Xov6OaOBIUCIiWFPrrjHGPOPY5Zyn/Rqsug97+yBPy4lOwH47u1rYGWPuMcacYIypj1X5+aUx5krgK6y56iHwXhTJueyNMduBzSJyomfTucBqiuH3AquIqpOIlPL8vdj3oth9L/xE+12YB3QTkYqeXFw3z7bg4l2xo6+gFV5dsLKLPwErPK8LsMpkvwB+9/xbyXO8AGOB9cAqrJYmcf8cMbgvZwGzPcsNgUXAOuADIN2zvYRnfZ1nf8N4pzuf78HJwBLPd2M6ULG4fi+Ah4BfgZ+Bt4H04vS9ACZj1e9kYOUchuTmuwAM9tyXdcB14a6rQ44opZSKihZVKaWUiooGDqWUUlHRwKGUUioqGjiUUkpFRQOHUkqpqGjgUMWWiBz0/FtfRK7I53OP8Fv/Pj/Pr1Q8aeBQyhokLqrAISLJYQ7xCRzGmM5RpkmphKWBQyl4HOgqIis88zski8hTIrLYM2/BjQAicpZYc6RMwupAhYhMF5Glnjkhhnq2PQ6U9JzvXc82O3cjnnP/LCKrRGSA49zzJWeejXc9vaGVSjgp4Q9Rqsi7G7jDGNMLwBMA9htjOohIOrBARD71HNsRaGmM2eBZH2yM2SMiJYHFIjLNGHO3iAwzxpzscq1LsXp+twGqeN7zjWdfW6AF1jhBC7DG6Pou/z+uUnmjOQ6lAnXDGtNnBdZQ9pWxJr8BWOQIGgD/FpGVwI9YA8U1IbQuwGRjTJYx5m/ga6CD49xbjDHZWEPM1M+XT6NUPtMch1KBBLjVGOMz0JuInIU1jLlz/TysyYEOi8h8rPGQwp07mGOO5Sz071MlKM1xKAUHgLKO9XnAzZ5h7RGRpp7JkvyVx5qK9LCInIQ1xa8tw36/n2+AAZ56lKrAGVgD7ilVaOgvGqWsUWYzPUVOb2LN6V0fWOapoN4JXOzyvk+Am0TkJ6xpmvZ+bQAAAF1JREFUOH907HsV+ElElhlrCHjbR1jTma7EGv34TmPMdk/gUapQ0NFxlVJKRUWLqpRSSkVFA4dSSqmoaOBQSikVFQ0cSimloqKBQymlVFQ0cCillIqKBg6llFJR+X/ZwlBpaO4+ZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ↑のSGD での train loss の推移を描画\n",
    "\n",
    "plt.plot(range(1, iters_num+1), train_loss_list, \"-\", color=\"navy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"training batch Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, iters_num+1), train_loss_list, \"-\", color=\"navy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"training batch Loss\")\n",
    "plt.title(\"Learning Curve (zoom)\")\n",
    "plt.xlim(1, 1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1エポックごとの train / test Accuracy を描画 (さっき標準出力したけど)\n",
    "\n",
    "plt.plot(range(1, len(train_acc_list)+1), train_acc_list, \"-\", label=\"train acc\")\n",
    "plt.plot(range(1, len(test_acc_list)+1), test_acc_list, \"--\", label=\"test acc\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlim(1, 8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by kNN : 0.961\n",
      "CPU times: user 1min 48s, sys: 735 ms, total: 1min 49s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# k-Nearest Neibhor Classifier と比較してみる． 計算重いからテストは軽く．ハイパラ k も適当\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors=5)\n",
    "knc.fit(x_train, np.argmax(t_train,axis=1))\n",
    "\n",
    "pred = knc.predict(x_test[:1000, :])\n",
    "print(\"Accuracy by kNN : \" + str(np.mean(pred == np.argmax(t_test[:1000, :], axis=1))) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 補足メモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD には，計算効率意外にも色々とメリットがある．特に大きいのは，MLP本を引用すると，\n",
    "\n",
    "> さらに確率的勾配降下法を使うと，反復計算が望まない（誤差関数の値が相対的にそれほど小さくない）局所的な極小解にトラップされてしまうリスクを低減できます．バッチ学習の場合，最小化しようとする目的関数は常に同じなので，望まない局所極小解に一旦トラップされてしまうと，二度とそこから抜け出せません．一方で確率的勾配降下法では，目的関数は $w$ の更新ごとに異なるので，そのようなリスクが小さくなります．反復のたびにランダムにサンプルを選べば，その効果は最大化できると考えられ，確率的勾配降下法の名前はここに由来します．\n",
    "\n",
    "て感じ． [ここ](https://www.slideshare.net/MotokawaTetsuya/optimizer-93979393) のスライド 3 の図もわかりやすい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "普通の数値最適化で定番なのはニュートン法とかだが，ヘッセ行列を求めるのが相当キツかったり正則じゃないかもしれない等の理由で，NNではあまり使われない．\n",
    "[ここ](https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning) に色々とディスカッションしてる方々がいる．[このスライド](http://www.dais.is.tohoku.ac.jp/~shioura/teaching/mp13/mp13-13.pdf) で短所として紹介されている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
