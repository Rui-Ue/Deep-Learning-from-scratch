# ７章  畳み込みニューラルネットワーク





### これまでのシンプルな NN の限界


- アーキテクチャは「Affine - Activate - ... - Affine - Activate - Affine - Final Activate」という並び
- Affine レイヤの入力はベクトル (ランク1テンソル) なので、各ピクセルの輝度値の行列 (カラーならランク3テンソル) を vec で潰してベクトルにして、扱うしかない。
- よって、このシンプルな NN では入力画像データの形状に関する情報 (隣り合うピクセルの輝度値はどれか、RGBのどの輝度値か) を使うことはできない。





### 何が問題か？

- Affine レイヤによって作られる各特徴量は「入力変数を全て使った (全ての入力変数を線形結合した)」ものである。
- その意味で「全結合 (fully-connected) レイヤ」と呼ばれたりもする。
- 学習がとてもうまくいけば「形状情報を考慮した特徴量」が勝手に作られるかもしれない。だが、fully-connected を想定したモデルなので、NN からすると各入力変数の位置関係は分からないので、なかなか難しい。





### じゃあどうするか？

- fully-connected で学習に任せるのではなく、明示的 (強制的) に「位置的に近いピクセルだけをもとに作られる特徴量」をモデルに仮定しておく。
- 例えば、何らかの「画像処理におけるフィルタリングやプーリング」をするレイヤをネットワークに追加すれば、強制的に「形状情報を考慮した特徴量」が作られることになる。
- どういうフィルタリングをするか、という内容についてはパラメータとして学習させる。

平均プーリング：[参考](https://tech-blog.s-yoshiki.com/entry/123)
![fig](https://images-tech-blog.s-yoshiki.com/img/2019/05/20190518004055.png)

ソーベルフィルタ：[参考](https://imagingsolution.net/imaging/filter-algorithm/)
![fig](https://imagingsolution.net/wordpress/wp-content/uploads/2011/03/blog163_10_median7_71.png)![fig](https://imagingsolution.net/wordpress/wp-content/uploads/2011/03/blog163_11.png)





### フィルタリングするための畳み込み層

- CNN では「畳み込み演算」をモデルに仮定 (ネットワークにレイヤ追加) することで、画像処理のフィルタリングを組み込んでいる。

- 演算の詳細は図 7-3, 7-4 がわかりやすい。

- フィルター行列の要素値が「どういうフィルタリングをするのか？」を表す。 CNN ではここはパラメータとして、データから学習させる。

  > [p208] CNN の場合、フィルターのパラメータが、これまでの「重み」に対応 します。そして、CNN の場合もバイアスが存在します。

- つまり俗っぽく言うと「職人がこれまで緻密に行ってきたフィルタリングを、AI がデータから自動で行う」みたいなイメージ。特徴量自体を学習する。
- 冷静に考えると、次のような違いがあるだけ。
  - Affine レイヤ：全ての入力変数の線形結合を何個か作る。fully-connected。
  - Conolution レイヤ：隣り合う一部の入力変数だけの線形結合を何個か作る。partially-connected。





### フィルタリング後の画像サイズ

- パディングを作ってやると、フィルタリング後の画像サイズが小さくなりすぎないようにする。図7-6。そうしないと、ディープにしたときに困る。
- 一方、ストライドを大きくすると、フィルタリング後の画像サイズが小さくなる。
- つまり結局、出力画像のサイズは以下の３つで決まる。具体的な計算式は (7.1)。このあたりは基本ハイパラで、学習はさせない。
  - フィルタ行列のサイズ
  - パディングの大きさ
  - ストライドの大きさ





### カラー画像 (複数チャネル画像) のフィルタリング

- カラー＝複数チャネル画像は、数学的にはランク 3 テンソルで表せる。例えば、256 x 256 の RGB 画像なら、サイズ (3, 256, 256) のテンソル。
- CNN の畳み込みでは、入力画像のチャネル数と同じチャネル数のフィルタテンソルを用意して、フィルタリングを行う。図 7-8, 7-9, 7-10。
- 出力画像は1チャネルつまり行列 (ランク2テンソル) になる。特徴マップと呼ぶ。
- 先ほどと同様に「Affine で fully-connected に作っていた線形結合特徴量を、partially-connected に作るように変更しただけ」とも捉えられる。





### 同じ画像に色んなフィルタをかけて色んな特徴取り出す

- [このサイト](https://imagingsolution.net/imaging/filter-algorithm/)からわかるように、色んなフィルタリング方法 (フィルタ行列)があって、全然違う特徴が捉えられる。
- なので、同じ入力画像に複数のフィルタリングを施すようにした方が、多様な特徴を学習できて良いだろう。そこで、ランク 3 フィルタテンソルをたくさん用意して、１個ずつ入力画像に適用して、それぞれで出力される１枚の画像行列を、チャネル方向に重ねて保持する。図 7-11, 7-12。
- ここで「たくさんのランク 3 フィルタテンソル」を「１つのランク 4 フィルタテンソル」として捉えている。





### バッチ処理

- ここまでの説明は全て、１枚の入力画像に対する話。
- 複数枚の画像に対して一気に予測する時や、back propagation で勾配計算を行う時には、バッチ処理したくなる。
- 入力をランク 4 テンソルと見なす。サイズは (バッチサイズ, チャネル数, 横ピクセル数, 縦ピクセル数) となる。図 7-13。
- あとは numpy 等で頑張る。





### プーリング層も使う

- 画像認識 (例えばイヌネコ分類) に使うことをイメージすると分かるように、なるべく「画像のズレ (微小な平行移動) に対してロバスト (値が変わりづらい)」な特徴量が欲しい。
- 学習がとてもうまくいけば、畳み込み層のフィルタで「ズレにロバストな特徴量」が得られる可能性も、一応ある。
- が、基本的に常にこのような特徴量は欲しいので、だったら最初からモデルに組み込んでしまえば良い。
- それがプーリング層。よく使われるのは、Max プーリング。あるピクセル範囲の最大値を取り出すので、明らかに、画像のズレにロバスト。図 7-14, 7-16。

![fig](https://images-tech-blog.s-yoshiki.com/img/2019/05/20190518004109.png)





### Convolution レイヤと Pooling レイヤの比較

- どちらも「隣り合うピクセル群から特徴を取り出す」という意味では同じ。
- Convolution レイヤでは、フィルタ行列の要素値をパラメータとすることで「どのようなフィルタリングが効果的か？」もデータから学習させる。
- Pooling レイヤには、学習するパラメータがない。というか、どういう Pooling　をすべきかをパラメータ化しても良いのだが、事前に「Max プーリングをすればズレにロバストな特徴量が得られて嬉しい」とわかっているので、わざわざ学習させる必要もない。
- Convolution レイヤによるフィルタリング後にはチャネル数を 1 にするが、Pooling レイヤではチャネル数をそのままにする。図 7-8 と 図 7-15 を比較。Pooling レイヤでチャネル数を 1 にまとめてしまうと、カラー画像に対して「ズレにロバストな特徴量」にならないから。





### CNN の全体アーキテクチャ

- 例えば図 7-2。シンプル NN の「Affine - Activate - ... - Affine - Activate - Affine - Final Activate」という並びの前に 「Convolution - Activate - Pooling」というレイヤセットををいくつか追加したもの。

- Pooling を使ったり使わなかったり、細かいバリエーションは色々ある。

- Goodfellow本では「１回でも Convolution してれば CNN」 という風にざっくりと定義している。

  > Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.





### CNN の実装

- フルスクラッチで実装するには...
  - back propagation の式を導出。ライプニッツルールをごりごり計算。
  - numpy でテンソルをうまく扱う方法を考えて、計算効率をなるべく下げる。
- いったん諦めて、PyTorch に頼る。





----





続きは `note_and_source.ipynb` へ。

