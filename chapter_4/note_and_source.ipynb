{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4章 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1. データから学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1. データ駆動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> コンピュータビジョンの分野で有名な特徴量としては、SIFT や SURF、HOG などが挙げられます。そのような特徴量を使って画像データをベクトルに変換し、その変換されたベクトルに対して、機械学習で使われる識別器―― SVM や KNN など――で学習させることができます。\n",
    "\n",
    "SIFT や SURF については山﨑先生の講義の第 7,8 回や [qiita](https://qiita.com/icoxfog417/items/adbbf445d357c924b8fc) を参照．回転やスケールにロバストな特徴量(検出器)の代表格．  \n",
    "つまり例えば，猫の耳が上下左右どっちから写ってても拡大縮小されててもどのくらい拡大縮小されて写ってても，同じような値になってくれる特徴量．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ただし、画像をベクトルに変換する際に使用した特徴量は、「人」が設計したものであることに注意が必要です。というのは、問題に応じて適した特徴量を使わなければ(もしくは特徴量 を設計しなければ)、なかなか良い結果が得られないのです。たとえば、犬の顔を見 分けるためには、「5」を認識する特徴量とは別の特徴量を人が考える必要があるかも しれません。つまり、特徴量と機械学習によるアプローチでも、問題に応じて、「人」 の手によって適した特徴量を考える必要があるかもしれないのです。\n",
    "\n",
    "ビジネス課題 PBL では「体感気温」とか「晴れてる時の気温」とかいう特徴量(交互作用的な)を自分で工夫して作って，それを使って回帰モデル組んで，精度向上(知見発掘)を目指した．まあ確かに，画像とかみたいに生の説明変数の意味が薄いようなデータでこれやるのは，大変．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ニューラルネットワークやディープラーニングは、従来の機械学習で使われた手法以上に、人の介入を遠ざけることができるという重要な性質を持ちます。\n",
    "\n",
    "> 図4-2 に示すとおり、ニューラルネットワークは、画像を“そのまま”学習します。 2 つ目のアプローチ――特徴量と機械学習によるアプローチ――の例では人が特徴量 を設計しましたが、ニューラルネットワークは，画像に含まれる重要な特徴量までも「機械」が学習するのです\n",
    "\n",
    "これ，よく言われる表現だが，まあどういう意味での主張か理解できた．  \n",
    "\n",
    "1. 例えば回帰のニューラルネットワークモデルを考える．  \n",
    "2. 一般的には，出力層での活性化関数は恒等関数．  すなわち，出力層のノード(アウトカムの値そのもの)は，最後の隠れ層ノードの線形結合になってる．  \n",
    "3. ここで，一番最後の隠れ層ノードっていうのは，マクロな視点で見ると，説明変数を線形変換・活性化で何度も何度も変換して得られた特徴量である．  データから交互作用をめっちゃ考慮した特徴量を作った感じ．  \n",
    "4. こう考えると，DNNは「重要な特徴量＝最終隠れ層ノード」を学習で獲得して，それで線形回帰している，とも捉えられる．\n",
    "\n",
    "このことから\n",
    "\n",
    "> ディープラーニングは、「end-to-end machine learning」と呼ばれること があります。ここで言う end-to-end とは、「端から端まで」という意味であ り、これは生データ(入力)から目的の結果(出力)を得ることを意味します。\n",
    "\n",
    "と言われてるそうだが，前処理(正規化とか白色化)が重要だっていう記述は前にあったし，ハイパラ調整の問題も残ってるし，まだまだ自動化しきれていないと思う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ニューラル ネットワークの学習で用いられる指標は、**損失関数** (loss function)と呼ばれます。この損失関数は、任意の関数を用いることができますが、一般には、2乗和誤差や交差エントロピー誤差などが用いられます。\n",
    "\n",
    "モデルのパラメータ推定のための最適化問題の目的関数のことを，一般に「損失関数(ロス関数，コスト関数)」って言うんだろうな．  \n",
    "交差エントロピーは使ったことないから理解しよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力(アウトカム)は多次元なので， (4.1) の２乗誤差は\n",
    "\n",
    "$$\n",
    "E_i = \\frac{1}{2} \\sum_{k=1}^K (y_{ik} - t_{ik})^2\n",
    "$$\n",
    "\n",
    "という感じで表すのが厳密かな．  \n",
    "ここで，$K$ は出力の次元 (ノード数) で，$y_{ik}$は $i$ 番目のケースに対する $k$ 番目の出力ノードの値 (確率の予測値)で，$t$ は同様に真のクラスをone-hot-encoding したもので $i$ 番目のケースがクラス $k$ に該当する時は $t_{ik} = 1$ でそれ以外では $0$．こう表した $E_i$ が，$i$ 番目のデータ(観測値,ケース)に対する二乗誤差．  \n",
    "$\\frac{1}{2}$ はなんか計算上の都合だと思う，後でわかりそう．\n",
    "てか，\n",
    "\n",
    "> $y_k$ はニューラルネットワークの出力、$t_k$ は教師データを表し、\n",
    "\n",
    "っていうノーテーションは PRML と同じだな．俺いつも $\\widehat{y}, y$ を使ってたが，まあハットめんどいしこっちのが楽だな． True の T だし．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2. 交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "２乗誤差の時と同様，出力ノードが多次元なので，\n",
    "\n",
    "$$\n",
    "E_i = - \\sum_{k=1}^K t_{ik} \\log y_{ik}\n",
    "$$\n",
    "\n",
    "が $i$ 番目のデータ (ケース) に対する予測の交差エントロピー．  \n",
    "PRML の p208,209 の (4.108) では「多クラス分類に対する交差エントロピー誤差」という名で同じ定義がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「交差エントロピー」という名の意味について考えていく．\n",
    "\n",
    "まず $y_k$ がここではソフトマックス関数が返した値であることを思い出すと，交差エントロピーの定義に含まれる $-\\log y_k$ は「確率 $y_k$ の値(事象)が観測された際に得られる情報量」である (PRLM の 1.92 式)．本来なら「得られる情報量 $-\\log y_k$ の期待値」がエントロピーだが (PRML の 1.93 式)，まあその辺の区別せず交差\"エントロピー\"という名前になってるのだと思う．\n",
    "\n",
    "PRML の (4.90) にはロジスティックにおける交差エントロピー誤差関数の定義があって，多クラス分類での交差エントロピー(PRML の 4.108)　と見た目違うがやってることは全く同じ．なぜなら，ロジスティックでは $t=0,1$ で2つのクラスを表しているので，(4.90)は「真のクラスが$t=1$のときは $y$ の情報量を足し込んで，真のクラスが $t=0$ のときは $1-y$ の情報量を足しこむ」という操作をやってて，これを多クラスにそのまま拡張したのが (4.108) だから．  \n",
    "そんで2クラスverの交差エントロピー (4.90) を見ると， クラスベクトル $t$ と出力ベクトル $y$ が交差(クロス)している感じがして， \"交差\"エントロピーという名も納得．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# エントロピー(情報量) log(p) のグラフ\n",
    "# 珍しい値(事象)ほど観測された時に得られる情報が多い，という思想．PRML参照．\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.001, 1, 0.001)\n",
    "y = - np.log(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"- \\log p\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> そのため、式 (4.2) つまり交差エントロピー誤差は実質的に正解ラベルが 1 に対応する出力の自然対数 (情報量) を計算するだけになります。\n",
    "\n",
    "ていう事実と↑のグラフから考えると，交差エントロピーは損失関数としてはまあ使えそうだな．  \n",
    "学習の時に微分が使いやすいとか，そういうメリットも後で紹介されるんだと思う．  \n",
    "\n",
    "まあ強引にクロスエントロピー誤差(の全ケース分の和)の意味を解釈するとしたら「予測(分類)を行った全ケースの真のクラスを観測した時の意外性(驚き度合い, え予測外れてんじゃん度合い)の合計」という感じか．\n",
    "\n",
    "あと気になる点としては，出力ノードの値が $y < 0$ となった場合はどうしてんだろう．あ，最後の活性化関数にソフトマックスを使えば心配無用か．  \n",
    "けど，↓の実装では 0 が渡される可能性を考慮して対策してるぽい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.11809565095832"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.log(0)   # -inf が返される．これだと，定義式中で 0 * -inf の不定形が生じて死ぬかも．\n",
    "np.log(1e-7)  # 超微小の値を足す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy_error(y, t):\n",
    "#     return - np.sum(t * np.log(y))\n",
    "\n",
    "def cross_entropy_error(y, t):  \n",
    "    delta = 1e-7\n",
    "    return - np.sum(t * np.log(y + 1e-7))\n",
    "# t=0,y=0 で 0*-inf の不定形が生じるのを防ぐため．\n",
    "# ソフトマックスを噛ませばこういう心配無いと思うが..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3. ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 先ほど説明した損失関数の例は、ひとつのデータの損失関数を考えてい ました。そこで、訓練データすべての損失関数の和を求めたいとすると、たとえば、 交差エントロピー誤差の場合、次の式 (4.3) のように書くことができます。\n",
    "$$\n",
    "E = - \\frac{1}{N} \\sum_n \\sum_k t_{nk} \\log y_{nk}\n",
    "$$\n",
    "\n",
    "あ，なんだ，この話ここで出てくるのか．  \n",
    "PRML の p48 を参照．エントロピー(てか情報量)をそのまま足し合わせると言うことは，情報理論的には各観測値の $y_i$ を独立と思っている感じになる．\n",
    "\n",
    "> 最後に N で割って正規化しています。こ の N で割ることによって、1 個あたりの「平均の損失関数」を求めることになりま す。そのように平均化すれば、訓練データの数に関係なく、いつでも統一した指標が 得られます。たとえば、訓練データが 1,000 個や 10,000 個の場合であっても、1 個 あたりの平均の損失関数を求められます。\n",
    "\n",
    "これやっても最適化問題としては変わらない気がするので，まあ，どういうモチベかわからん．ミニバッチ学習の時に必要？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> すべてのデータを対象とした損失関数を計算 するのは、現実的ではありません。そこで、データの中から一部を選び出し、その一 部のデータを全体の「近似」として利用します。ニューラルネットワークの学習にお いても、訓練データからある枚数だけを選び出し――これをミニバッチ(小さな塊) と言う――、そのミニバッチごとに学習を行います。たとえば、60,000 枚の訓練デー タの中から 100 枚を無作為に選び出して、その 100 枚を使って学習を行うのです。 このような学習手法をミニバッチ学習と言います。\n",
    "\n",
    "こういう問題意識は NN 特有な感じがして，これまで触れてこなかった．  \n",
    "\n",
    "線形回帰のパラメータ推定で強引に例えてみる．  \n",
    "目的関数は $(y - X \\beta)^T (y - X \\beta)$ で，これは $ \\beta = (\\beta_1, \\beta_2, \\ldots, \\beta_p)^T$ 関数．巨大な $X, y$ から一部の行(観測値)を抜き出して $X_s, y_s$ を作り(これがミニバッチ)，本来最適化したい $(y - X \\beta)^T (y - X \\beta)$ を計算が軽く済む $(y_s - X_s \\beta)^T (y_s - X_s \\beta)$　で近似して，こっちについて最適化を行う．そうすると，ちゃんとランダムサンプリングできていれば，低い計算コストで (平均的には) 同じ値の $\\widehat{\\beta}$ をゲットできる．  \n",
    "イメージ的には例えば，よく見る 2変数ロジスティックの対数尤度関数の山を，一部のサンプルだけを見て作った山で近似する感じ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape, t_train.shape, x_test.shape, t_test.shape)\n",
    "# one_hot_label=True　で，数字ラベルを one hot encoding で取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 6 9 8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.random.choice(10, 3))\n",
    "%R print( sample(0:9, 3, replace=F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "idx = np.random.choice(x_train.shape[0], batch_size)\n",
    "x_batch = x_train[idx, :]\n",
    "t_batch = t_train[idx, :]\n",
    "print(x_batch.shape, t_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0]\n",
      "1\n",
      "(4,)\n",
      "[[0 1 0 0]]\n",
      "2\n",
      "(1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2039724376593033"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 入力として，\n",
    "# - ndim=1 の array (単一の観測値)\n",
    "# - ndim=2 の array (ミニバッチ)\n",
    "# の両方を処理できるようにしたいので，どうするかというと...\n",
    "\n",
    "y = np.array([0, 1, 0, 0])\n",
    "print(y)\n",
    "print(y.ndim)\n",
    "print(y.shape)\n",
    "\n",
    "y_2d = y.reshape(1, y.shape[0])\n",
    "print(y_2d)\n",
    "print(y_2d.ndim)\n",
    "print(y_2d.shape)\n",
    "\n",
    "y = np.array([[0.1, 0.3, 0.6], [0.5, 0.4, 0.1]])\n",
    "t = np.array([[0, 0, 1], [1, 0, 0]])\n",
    "t*y\n",
    "t*np.log(y + 1e-7)                                    # 0が入力された時用\n",
    "- np.sum(t*np.log(y + 1e-7), axis=1)                 # 丁寧に観測値ごとにクロスエントロピーロスを算出して，\n",
    "np.sum(- np.sum(t*np.log(y + 1e-7), axis=1)) # それを足し合わせる．\n",
    "- np.sum(t*np.log(y + 1e-7))                              # けどまあ，面倒だから一気に全部足してOK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:  # ndim=1(ベクトル) つまり単一観測値が入力されたら ndim=2(行列) に整形\n",
    "        y = y.reshape(1, y.shape[0])\n",
    "        t = t.reshape(1, t.shape[0])\n",
    "    return - (1/y.shape[0]) * np.sum(t * np.log(y + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6019862188296516"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[0.1, 0.3, 0.6], [0.5, 0.4, 0.1]])\n",
    "t = np.array([[0, 0, 1], [1, 0, 0]])\n",
    "# y = np.array([0.1, 0.3, 0.6])\n",
    "# t = np.array([0, 0, 1])\n",
    "\n",
    "cross_entropy_error(y=y, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01', '12'], dtype='<U2')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# クラスが one-hot じゃなくてラベルで与えられた時の実装方法を考えるため...\n",
    "\n",
    "mat = np.array([[\"00\", \"01\", \"02\"], [\"10\", \"11\", \"12\"], [\"20\", \"21\", \"22\"]])\n",
    "mat[[0, 1], [1, 2]]\n",
    "\n",
    "# おお，こんな感じで「 i 行 j　列と k　行　ｌ 列と... を持ってきてね」という抽出ができるのか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_label(y, t):\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, y.shape[0])\n",
    "    return - (1/y.shape[0]) * np.sum(np.log(y[np.arange(y.shape[0]), t] + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6019862188296516"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[0.1, 0.3, 0.6], [0.5, 0.4, 0.1]])\n",
    "t = np.array([2, 0])\n",
    "# y = np.array([0.1, 0.3, 0.6])\n",
    "# t = 2\n",
    "# t = np.array([2])\n",
    "\n",
    "cross_entropy_error_label(y=y, t=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.5. なぜ損失関数を設定するのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 私たちが目標とすることは、できるだけ認識精度が高くなるニューラルネッ トワークを獲得することなので、「認識精度」を指標にすべきではないか、という疑 問です。\n",
    "\n",
    "たしかに，言われてみればこの疑問は自然．実際，決定木だと (推奨はジニ係数とかエントロピーだけど) 誤判別率を不純度として使うことあるよね．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この 4.2.5. の話，全体的に面白い．まとめると...\n",
    "\n",
    "1. 明らかに，認識精度(誤分類率) は，ある重みパラメータをちょっと動かしただけだと，全く変化しない．\n",
    "2. つまりパラメータでの微分 (誤分類率の勾配) が，大半の場所で 0 となる．となると，勾配降下(学習, パラメータ探索)できない．\n",
    "3. 活性化関数が滑らかでないときも，損失関数が滑らかでなくなってしまうので，1.2.と全く同じく学習ができなくなってしまう．\n",
    "\n",
    "この説明だと，ステップじゃなくてシグモイド(非線型関数)を使うことの理由も人に説明しやすそう．ニューラルネットワーク内の関数のどこか１つでも滑らかじゃない関数があると，そのせいで損失関数も滑らかじゃなくなって，勾配降下できなくなる．\n",
    "\n",
    "まあもっと砕いた説明をすると，損失関数を容器みたいな感じで見て，そこにボールを入れて底まで届かせることをイメージすれば良い．傾きが０のところがあったら，底までいけない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3. 数値微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "# 解析的に微分すると 0.02*x + 0.1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(0, 20, 0.1)\n",
    "y = function_1(x)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値微分のダメな実装例\n",
    "\n",
    "def numerical_diff_not(f, x):\n",
    "    h = 1e-50                              # h を 0 に近づけすぎていて丸め誤差の問題発生\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.0 2.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print( numerical_diff_not(function_1, 5) )\n",
    "print( numerical_diff_not(function_1, 10) )\n",
    "print( numerical_diff_not(function_1, 100) )\n",
    "print( numerical_diff_not(function_1, 1000) )\n",
    "\n",
    "print(function_1(10+1e-50), function_1(10))\n",
    "print(function_1(10+1e-50) - function_1(10))\n",
    "\n",
    "# 解析的に求められた勾配と全然一致しない．\n",
    "# 丸め誤差(ビット数不足でどこかでほんのわずかにある差が丸められて0になってしまった)が原因で微分0．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値微分の良い例\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4                                               # 丸め誤差対策で h は大きめ\n",
    "    return (f(x+h) - f(x-h)) / (2*h)         # 近似精度上げるため前方差分でなく中心差分を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n",
      "2.1000000001691888\n",
      "20.099999992453377\n",
      "2.0000300000999998 1.9999700001\n",
      "5.999999999972694e-05\n"
     ]
    }
   ],
   "source": [
    "print( numerical_diff(function_1, 5) )\n",
    "print( numerical_diff(function_1, 10) )\n",
    "print( numerical_diff(function_1, 100) )\n",
    "print( numerical_diff(function_1, 1000) )\n",
    "\n",
    "print(function_1(10 + 1e-4), function_1(10 - 1e-4))\n",
    "print(function_1(10 + 1e-4) - function_1(10 - 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3. 偏微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "#     return np.sum(x**2)\n",
    "\n",
    "\n",
    "# f(x) = x0^2 + x1^2\n",
    "# グラフの形は式から明らか．x0,x1いずれかを固定(定数化)すれば 2次関数x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378 7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "# x0=3, x1=4 における勾配を1変数微分 function だけ使って求める． \n",
    "\n",
    "def tmp0(x0):  # x1=4 で固定してバサッと切った断面\n",
    "    return function_2(np.array([x0, 4]))\n",
    "\n",
    "def tmp1(x1):  # x0=3 で固定してバサッと切った断面\n",
    "    return function_2(np.array([3, x1]))\n",
    "\n",
    "\n",
    "print( numerical_diff(tmp0, 3), numerical_diff(tmp1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4. 勾配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "[0 0 0]\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print( np.zeros(3) )\n",
    "\n",
    "tmpvec = np.array([1,2,3])\n",
    "tmpmat = np.array([[1,2,3], [4,5,6]])\n",
    "\n",
    "print( np.zeros_like(tmpvec)  )\n",
    "print( np.zeros_like(tmpmat) )\n",
    "\n",
    "# np.zeros_like() ってめっちゃ便利だな．こういう操作すること多い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(  np.array([1,2,3]).size                   )\n",
    "print(  np.array([[1,2,3],[4,5,6]]).size  )\n",
    "# ndim 問わず全要素数が返されるのか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    \n",
    "    h = 1e-4                                  # 丸め誤差対策で小さくしすぎない\n",
    "    grad = np.zeros_like(x)         # x(変数ベクトル)と同じ形の array を生成．これを勾配に仕上げる\n",
    "    \n",
    "    for idx in range(x.size):         # 変数ベクトルを1次元ずつループ\n",
    "        \n",
    "        tmp = x[idx]                 # x[idx] を逃しておく\n",
    "        \n",
    "        x[idx] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)   # 中心差分で偏微分係数を近似算出して記録\n",
    "        \n",
    "        x[idx] = tmp                # x[idx] を元に戻す\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "\n",
    "# numerical_gradient(function_2, np.array([3, 4]))\n",
    "# え，こっちだと全然ダメな結果になった．ちゃんと .0 を打って float として扱わせないとやばいのか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[4. 6.]\n",
      "[0. 2.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 各地点の勾配を見ながらセルフ勾配降下\n",
    "# 最小化(降下)したいので，勾配がプラスなら負の方向に動いて，マイナスなら正の方向に動くべき\n",
    "\n",
    "print( numerical_gradient(function_2, np.array([3.0, 4.0])) )\n",
    "print( numerical_gradient(function_2, np.array([2.0, 3.0])) )\n",
    "print( numerical_gradient(function_2, np.array([0.0, 1.0])) )\n",
    "print( numerical_gradient(function_2, np.array([0.0, 0.0])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ch04/gradient_2d.py\n",
    "\n",
    "# 勾配を矢印で表した図 4-9 を生成するための関数を，今後も自分で使いやすいようにカスタマイズしたい．\n",
    "# 例えばロジスティックの勾配降下を説明する時とかに便利そう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
